{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Synthetic data for SF6\n",
    "\n",
    "The only \"objective\" way we have to assess the (potential) quality of our posterior fluxes is by constructing observations based on known emissions.\n",
    "\n",
    "First, let's see what prior data we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import openghg\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openghg.retrieve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_res = search_flux(species=\"sf6\", domain=\"europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_path = Path(\"/group/chem/acrg/Gridded_fluxes/SF6/EDGAR_v8.0/\")\n",
    "flux_path_yearly = flux_path / \"yearly\"\n",
    "flux_path_yearly_sectoral = flux_path / \"yearly_sectoral\"\n",
    "\n",
    "!ls {flux_path_yearly} | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {flux_path_yearly_sectoral}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "It seems these fluxes aren't stored anywhere, so we'll have create a store..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_path = Path(\"/group/chem/acrg/object_stores/\")\n",
    "!ls {stores_path}/paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from openghg.objectstore import get_readable_buckets\n",
    "pprint(get_readable_buckets())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Maybe I can just look at the raw files first..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## EDGAR fluxes for SF6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_files = !ls {flux_path_yearly}\n",
    "yearly_files[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed2022 = xr.open_dataset(flux_path_yearly / yearly_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.util import find_domain\n",
    "lat, lon = find_domain(\"europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_slice = slice(lat.min(), lat.max())\n",
    "lon_slice = slice(lon.min(), lon.max())\n",
    "np.log(ed2022.fluxes.sel(lat=lat_slice, lon=lon_slice)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Looks okay... let's zoom in on Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(ed2022.fluxes.sel(lat=slice(35, 60), lon=slice(-15, 20))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_cube = ed2022.fluxes.to_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct construction\n",
    "from iris.coords import DimCoord\n",
    "from iris.cube import Cube\n",
    "\n",
    "ed_lat = ed2022.coords[\"lat\"].values\n",
    "ed_lon = ed2022.coords[\"lon\"].values\n",
    "ed_cube_lat = DimCoord(ed_lat, standard_name=\"latitude\", units=\"degrees\")\n",
    "ed_cube_lon = DimCoord(ed_lon, standard_name=\"longitude\", units=\"degrees\")\n",
    "ed_cube2 = Cube(ed2022.fluxes.values, dim_coords_and_dims=[(ed_cube_lat, 0), (ed_cube_lon, 1)])\n",
    "ed_cube2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ed_cube2.coord(\"latitude\").guess_bounds()\n",
    "ed_cube2.coord(\"longitude\").guess_bounds()\n",
    "ed_cube2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_out, lon_out = find_domain(\"europe\")\n",
    "nlat = len(lat_out)\n",
    "nlon = len(lon_out)\n",
    "out_da = xr.DataArray(np.zeros((nlat, nlon), dtype=\"float32\"), coords=[lat_out, lon_out], dims=[\"latitude\", \"longitude\"])\n",
    "out_da.latitude.attrs[\"units\"] = \"degrees\"\n",
    "out_da.longitude.attrs[\"units\"] = \"degrees\"\n",
    "out_cube = out_da.to_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct method for making output cube... this mattered for some reason\n",
    "cube_lat_out = DimCoord(lat_out, standard_name=\"latitude\", units=\"degrees\")\n",
    "cube_lon_out = DimCoord(lon_out, standard_name=\"longitude\", units=\"degrees\")\n",
    "out_cube2 = Cube(\n",
    "    np.zeros((len(lat_out), len(lon_out)), np.float32),\n",
    "    dim_coords_and_dims=[(cube_lat_out, 0), (cube_lon_out, 1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cube2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cube == out_cube2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cube.coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cube2.coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros_from_domain(domain: str) -> xr.DataArray:\n",
    "    \"\"\"Make a DataArray of zeros with lat/lon coords from given domain.\"\"\"\n",
    "    lat_out, lon_out = find_domain(domain)\n",
    "    nlat = len(lat_out)\n",
    "    nlon = len(lon_out)\n",
    "    da_out = xr.DataArray(np.zeros((nlat, nlon)),\n",
    "                          coords=[lat_out, lon_out], \n",
    "                          dims=[\"latitude\", \"longitude\"])\n",
    "    da_out.latitude.attrs = {\"units\": \"degrees\", \"standard_name\": \"latitude\"}\n",
    "    da_out.longitude.attrs = {\"units\": \"degrees\", \"standard_name\": \"longitude\"}\n",
    "    \n",
    "    return da_out\n",
    "\n",
    "\n",
    "def _regrid_2d(\n",
    "    input_data: xr.DataArray,\n",
    "    in_lat_coord: str = \"lat\",\n",
    "    in_lon_coord: str = \"lon\",\n",
    "    domain: str = \"EUROPE\",\n",
    ") -> xr.DataArray:\n",
    "    from iris.analysis import AreaWeighted\n",
    "\n",
    "    cube_out = zeros_from_domain(domain).to_iris()\n",
    "    cube_out.coord(\"latitude\").guess_bounds()\n",
    "    cube_out.coord(\"longitude\").guess_bounds()\n",
    "\n",
    "    cube_in = input_data.rename({in_lat_coord: \"lat\", in_lon_coord: \"lon\"}).to_iris()\n",
    "    cube_in.coord(\"latitude\").guess_bounds()\n",
    "    cube_in.coord(\"longitude\").guess_bounds()\n",
    "\n",
    "    cube_regridded = cube_in.regrid(cube_out, AreaWeighted(mdtol=1.0))\n",
    "\n",
    "    result = xr.DataArray.from_iris(cube_regridded).rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "    result.attrs = input_data.attrs\n",
    "    result.attrs.pop(\"ChunkSizes\", None)\n",
    "    \n",
    "    return result    \n",
    "\n",
    "def regrid_2d(\n",
    "    input_data: xr.Dataset | xr.DataArray,\n",
    "    in_lat_coord: str = \"lat\",\n",
    "    in_lon_coord: str = \"lon\",\n",
    "    in_data_var: str = \"flux\",\n",
    "    domain: str = \"EUROPE\",\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Regrid 2d data to given domain.\n",
    "\n",
    "    NOTE: only one data variable is regridded.\n",
    "\n",
    "    Args:\n",
    "        input_data: data to regrid\n",
    "        in_lat_coord: name of latitude coordinate for input_data\n",
    "        in_lon_coord: name of longitude coordinate for input_data\n",
    "        in_data_var: name of the data variabile to regrid\n",
    "        domain: name of domain to regrid to.\n",
    "    Returns:\n",
    "        xr.Dataset containing the regridded data.\n",
    "    \"\"\"\n",
    "    if isinstance(input_data, xr.Dataset):\n",
    "        data_in = input_data[in_data_var]\n",
    "    else:\n",
    "        data_in = input_data\n",
    "\n",
    "    result = _regrid_2d(data_in, in_lat_coord, in_lon_coord, domain).to_dataset()\n",
    "    result.attrs = input_data.attrs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed2022_regridded = regrid_2d(ed2022, in_data_var=\"fluxes\").rename(fluxes=\"flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed2022_regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(ed2022_regridded.flux).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.standardise.meta import define_species_label\n",
    "from openghg.util import molar_mass\n",
    "\n",
    "sf6_molar_mass = molar_mass(define_species_label(\"sf6\")[0])  # why use \"define_species_label\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_molar_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "molar_mass(\"sF6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed2022_regridded.flux.max() / molar_mass(\"sf6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.util import cf_ureg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_molar_mass = molar_mass(\"sf6\") * cf_ureg.g / cf_ureg.mol\n",
    "sf6_molar_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_ureg.define(\"@alias kilogram = kg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_ureg((ed2022_regridded.pint.quantify() / sf6_molar_mass).pint.dequantify().flux.attrs[\"units\"]).to(\"mol/m2/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf_ureg.parse_unit_name(\"Mg\"))\n",
    "print(cf_ureg.parse_unit_name(\"Mg\", case_sensitive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.util._units import inverse_unit_mapping\n",
    "pprint(inverse_unit_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_edgar_flux_file(filepath: str | Path, domain: str, species: str | None = None, year: int | None = None, month: int = 1) -> xr.Dataset:\n",
    "    with xr.open_dataset(filepath, chunks={}) as ds:\n",
    "        species = species or ds.fluxes.attrs.get(\"substance\")\n",
    "        ds_regridded = regrid_2d(ds, domain=domain, in_data_var=\"fluxes\")\n",
    "\n",
    "    ds_regridded = ds_regridded.rename(fluxes=\"flux\")\n",
    "\n",
    "    if year is None:\n",
    "        try:\n",
    "            year = int(ds_regridded.flux.attrs[\"year\"])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "\n",
    "    if year is not None:\n",
    "        ds_regridded = ds_regridded.expand_dims({\"time\": [pd.Timestamp(year=year, month=month, day=1)]})\n",
    "\n",
    "    if species is None:\n",
    "        return ds_regridded\n",
    "\n",
    "    molmass = molar_mass(species) * cf_ureg.g / cf_ureg.mol\n",
    "\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        ds_regridded = (ds_regridded.pint.quantify() / molmass).pint.to(\"mol/m2/s\")\n",
    "\n",
    "    return ds_regridded.pint.dequantify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux2022 = transform_edgar_flux_file(flux_path_yearly / yearly_files[-1], \"europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux2022.flux.pint.quantify().max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# Making synthetic data from EDGAR fluxes\n",
    "\n",
    "## Storing EDGAR total fluxes for SF6\n",
    "We probably want to store the fluxes in an object store, since we'll need to compare with them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /group/chem/acrg/object_stores/paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /user/home/bm13805/.openghg/openghg.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.util._user import _add_path_to_config\n",
    "\n",
    "_add_path_to_config(\"group/chem/acrg/object_stores/paris/sf6_testing_store\", name=\"sf6_testing_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.standardise import standardise_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardise_flux?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Let's get the flux data regridded and keep it in memory, since we'll need it to make the synthetic obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes = [transform_edgar_flux_file(flux_path_yearly / yf, domain=\"europe\", species=\"sf6\") for yf in yearly_files[-10:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_ds = xr.concat(fluxes, dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_ds.flux.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile() as f:\n",
    "    flux_ds.to_netcdf(f, engine=\"h5netcdf\")\n",
    "    standardise_flux(f.name,\n",
    "                     domain=\"europe\", \n",
    "                     source=\"edgar-annual-total\", \n",
    "                     species=\"sf6\", \n",
    "                     database=\"edgar\", \n",
    "                     database_version=\"v8.0\", \n",
    "                     period=\"yearly\", \n",
    "                     if_exists=\"new\",\n",
    "                     store=\"sf6_testing_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_res = search_flux(species=\"sf6\", domain=\"europe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Making synthetic obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "sf6_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/\")\n",
    "sf6_base_nid2025_path = sf6_path / \"RHIME_NAME_EUROPE_FLAT_ConfigNID2025_sf6_yearly\"\n",
    "ini_files = !ls {sf6_base_nid2025_path / \"*.ini\"}\n",
    "# get 2015-2024\n",
    "ini_files = ini_files[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inversions_experimental_code/data_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = read_ini(ini_files[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dict(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_params, _ = split_function_inputs(params, MultiObs.__init__)\n",
    "obs_params[\"inlets\"] = params[\"inlet\"]\n",
    "obs_params[\"instruments\"] = params[\"instrument\"]\n",
    "obs_params[\"averaging_periods\"] = [\"1h\"] * len(obs_params[\"sites\"])\n",
    "obs_params[\"obs_data_levels\"] = [None] * len(obs_params[\"sites\"])\n",
    "pprint(obs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_2022 = MultiObs(**obs_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_params, _ = split_function_inputs(params, MultiFootprint.__init__)\n",
    "pprint(fp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_params[\"fp_heights\"] = params[\"fp_height\"]\n",
    "fp_params[\"met_model\"] = [None] * len(fp_params[\"sites\"])\n",
    "fp_params[\"model\"] = \"name\"\n",
    "pprint(fp_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_2022 = MultiFootprint(**fp_params, obs_data=obs_2022.obs, obs_sites=obs_2022.sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2022 = xr.DataTree.from_dict({k: v.data for k, v in fp_2022.footprints.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flux_mult(footprint: xr.Dataset, flux: xr.Dataset, ffill: bool = False) -> xr.Dataset:\n",
    "    if \"fp\" not in footprint.data_vars:\n",
    "        return footprint\n",
    "        \n",
    "    if ffill:\n",
    "        flux = flux.reindex_like(footprint, method=\"ffill\")\n",
    "    result = footprint.copy()\n",
    "\n",
    "    fp_x_flux = (footprint.fp.pint.quantify() * flux.flux.pint.quantify()).pint.dequantify()\n",
    "    result[\"mod_obs\"] = fp_x_flux.sum([\"lat\", \"lon\"]).astype(\"float32\")\n",
    "    result[\"mod_obs\"].attrs[\"units\"] = fp_x_flux.attrs[\"units\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_2022_obj = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\")\n",
    "flux_2022 = flux_2022_obj.data\n",
    "flux_2022_obj.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_2022.flux.max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2022 = dt_2022.map_over_datasets(flux_mult, flux_2022, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2022.MHD.mod_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2022.MHD.mod_obs.values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhd_2022_synth = dt_2022.MHD.mod_obs.pint.quantify().pint.to(\"ppt\").pint.dequantify().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhd_2022_synth.to_series().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"sf6_model_testing_data/\")\n",
    "merged_data = load_merged_data(data_path, start_date=\"2022\", species=\"sf6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "mhd_2022_synth.plot(ax=ax, label=\"synth\", alpha=0.5)\n",
    "obs_2022.obs[\"MHD\"].data.mf.plot(ax=ax, label=\"real\", alpha=0.5)\n",
    "fig.legend()\n",
    "#ax.set_ylim(10.6, 11.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.scenario.MHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesw_bc_basis(ds: xr.Dataset) -> xr.DataArray:\n",
    "    bc_ds = ds[[f\"bc_{d}\" for d in \"nesw\"]].rename({f\"bc_{d}\": d for d in \"nesw\"})\n",
    "    return bc_ds.sum([\"lat\", \"lon\", \"height\"]).to_dataarray(dim=\"bc_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = nesw_bc_basis(merged_data.scenario.MHD.dataset).sum(\"bc_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = baseline.reindex_like(mhd_2022_synth, method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_obs = merged_data.scenario.MHD.fp_x_flux.sum([\"lat\", \"lon\"])\n",
    "mod_obs = mod_obs.reindex_like(mhd_2022_synth, method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "(baseline + mhd_2022_synth - 0.02).resample(time=\"4h\").mean().plot(ax=ax, label=\"synth\", alpha=0.5)\n",
    "obs_2022.obs[\"MHD\"].data.mf.resample(time=\"4h\").mean().plot(ax=ax, label=\"real\", alpha=0.5)\n",
    "(mod_obs + baseline - 0.02).resample(time=\"4h\").mean().plot(ax=ax, label=\"prior\", alpha=0.5)\n",
    "fig.legend()\n",
    "ax.set_ylim(10.9, 11.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "\n",
    "mhd_2022_synth.plot(ax=ax, label=\"synth\", alpha=0.5)\n",
    "(obs_2022.obs[\"MHD\"].data.mf - baseline).plot(ax=ax, label=\"real - baseline\", alpha=0.5)\n",
    "mod_obs.plot(ax=ax, label=\"mod obs\", alpha=0.5)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline2 = obs_2022.obs[\"MHD\"].data.mf.resample(time=\"14D\").median()\n",
    "baseline2 = baseline2.reindex_like(mhd_2022_synth, method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "baseline.plot(ax=ax, label=\"prior baseline\", alpha=0.5)\n",
    "baseline2.plot(ax=ax, label=\"14D median baseline\", alpha=0.5)\n",
    "fig.legend()\n",
    "ax.set_ylim(10.9, 11.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "bias = 0.0 # -0.02\n",
    "(baseline2 + mhd_2022_synth + bias).plot(ax=ax, label=\"synth\", alpha=0.5)\n",
    "obs_2022.obs[\"MHD\"].data.mf.plot(ax=ax, label=\"real\", alpha=0.5)\n",
    "(mod_obs + baseline2 + bias).plot(ax=ax, label=\"prior\", alpha=0.5)\n",
    "fig.legend()\n",
    "ax.set_ylim(10.9, 11.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "## Adding uncertainty\n",
    "\n",
    "We could use the magnitude of the real obs minus the baseline to get an idea of the size of the errors we might want to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (obs_2022.obs[\"MHD\"].data.mf - baseline).to_series().describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_noise_like(da: xr.DataArray, sigma: float, seed: int | None = None) -> xr.DataArray:\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    noise = rng.normal(scale=sigma, size=da.shape).astype(\"float32\")\n",
    "    result = xr.DataArray(noise, coords=da.coords, dims=da.dims)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = make_noise_like(mhd_2022_synth, sigma=0.03, seed=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots()\n",
    "\n",
    "(noise + mhd_2022_synth - 0.02).resample(time=\"4h\").mean().plot(ax=ax, label=\"synth+noise\", alpha=0.5)\n",
    "(obs_2022.obs[\"MHD\"].data.mf - baseline2).resample(time=\"4h\").mean().plot(ax=ax, label=\"real - baseline\", alpha=0.5)\n",
    "#mod_obs.plot(ax=ax, label=\"mod obs\", alpha=0.5)\n",
    "#noise.plot(ax=ax, label=\"noise\", alpha=0.3)\n",
    "#obs_2022.obs[\"MHD\"].data.mf_repeatability.plot(ax=ax, label=\"mf repeatability\", alpha=0.3)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## Making synthetic obs\n",
    "\n",
    "- baseline: load merged data and get prior baseline\n",
    "- pollution events: load edgar flux, load multi_fp and map over data tree\n",
    "- uncertainties: take 1/2 IQR of obs for sigma (could also do no uncertainty, half this, and twice this)\n",
    "- mf_repeatability: make this constant, equal to value of sigma? or just copy from obs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\").data\n",
    "\n",
    "year = \"2015\"\n",
    "ini_file = next(ini_file for ini_file in ini_files if year in ini_file)\n",
    "params = read_ini(ini_file)\n",
    "\n",
    "obs_params, _ = split_function_inputs(params, MultiObs.__init__)\n",
    "obs_params[\"inlets\"] = params[\"inlet\"]\n",
    "obs_params[\"instruments\"] = params[\"instrument\"]\n",
    "obs_params[\"averaging_periods\"] = [\"1h\"] * len(obs_params[\"sites\"])\n",
    "obs_params[\"obs_data_levels\"] = [None] * len(obs_params[\"sites\"])\n",
    "multi_obs = MultiObs(**obs_params)\n",
    "\n",
    "fp_params, _ = split_function_inputs(params, MultiFootprint.__init__)\n",
    "fp_params[\"fp_heights\"] = params[\"fp_height\"]\n",
    "fp_params[\"met_model\"] = [None] * len(fp_params[\"sites\"])\n",
    "fp_params[\"model\"] = \"name\"\n",
    "multi_fp = MultiFootprint(**fp_params, obs_data=multi_obs.obs, obs_sites=multi_obs.sites)\n",
    "\n",
    "flux_sel = flux.sel(time=f\"{year}-01-01\")\n",
    "fp_dt = xr.DataTree.from_dict({k: v.data for k, v in multi_fp.footprints.items()}).map_over_datasets(flux_mult, flux_sel, True)\n",
    "\n",
    "def get_mod_obs(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"mod_obs\" not in ds.data_vars:\n",
    "        return ds\n",
    "    return ds[[\"mod_obs\"]].pint.quantify().pint.to(\"ppt\").pint.dequantify().compute()\n",
    "\n",
    "all_mod_obs = fp_dt.map_over_datasets(get_mod_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mod_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"sf6_model_testing_data/\")\n",
    "merged_data = load_merged_data(data_path, start_date=year, species=\"sf6\")\n",
    "\n",
    "def make_prior_baseline(ds: xr.Dataset) -> xr.Dataset:\n",
    "    if \"bc_n\" not in ds.data_vars:\n",
    "        return ds\n",
    "    return nesw_bc_basis(ds).sum(\"bc_region\").rename(\"baseline\").to_dataset()\n",
    "\n",
    "baselines = merged_data.scenario.map_over_datasets(make_prior_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dict = {k: v.to_dataset() for k, v in baselines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_obs_dict = {k: v.to_dataset() for k, v in all_mod_obs.items()}\n",
    "mod_obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_obs = {}\n",
    "std_cutoff = 0.7\n",
    "std_resample = \"8D\"\n",
    "std_scaling = 0.9\n",
    "for site, mod_obs in mod_obs_dict.items():\n",
    "    baseline = baseline_dict[site]\n",
    "    baseline = baseline.reindex_like(mod_obs, method=\"nearest\")\n",
    "    ds = xr.merge([mod_obs, baseline])\n",
    "\n",
    "    obs = multi_obs.obs[site.upper()].data\n",
    "    std = std_scaling * ((obs.mf\n",
    "        .where(obs.mf < obs.mf.quantile(std_cutoff).values, drop=True)\n",
    "        .resample(time=std_resample).std())\n",
    "#        .reindex_like(mod_obs.mod_obs, method=\"nearest\")\n",
    "        .median()\n",
    "        )\n",
    "    ds[\"noise\"] = make_noise_like(mod_obs.mod_obs, sigma=std.values, seed=123456789)\n",
    "    ds[\"noise\"].attrs[\"sigma\"] = std\n",
    "    \n",
    "    ds[\"sf6\"] = mod_obs.mod_obs + baseline.baseline\n",
    "    ds[\"sf6\"].attrs = obs.mf.attrs\n",
    "    ds.attrs = obs.attrs\n",
    "    ds[\"sf6_repeatability\"] = std * xr.ones_like(ds[\"sf6\"])\n",
    "    ds[\"sf6_repeatability\"].attrs[\"units\"] = df[\"sf6\"].attrs[\"units\"]\n",
    "    synth_obs[site] = ds.compute()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_obs.obs[\"MHD\"].data.mf.to_series().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_obs[\"MHD\"].noise.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## Plotting synthetic obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 8))\n",
    "to_subtract = [0, (baseline_dict[\"MHD\"].baseline -0.03).reindex_like(synth_obs[\"MHD\"], method=\"ffill\")]\n",
    "\n",
    "\n",
    "for to_sub, ax in zip(to_subtract, axs.flat):\n",
    "    (synth_obs[\"MHD\"].sf6 + synth_obs[\"MHD\"].noise - to_sub).plot(ax=ax, label=\"synth+noise\", alpha=0.3)\n",
    "    (multi_obs.obs[\"MHD\"].data.mf - to_sub).plot(ax=ax, label=\"obs\", alpha=0.3)\n",
    "    if not isinstance(to_sub, xr.DataArray):\n",
    "        to_subtract[1].plot(ax=ax, label=\"baseline\", alpha=0.3)\n",
    "    (synth_obs[\"MHD\"].sf6 - to_sub).plot(ax=ax, label=\"synth\", alpha=0.5)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_events = (multi_obs.obs[\"MHD\"].data.mf - to_subtract[1]).to_series().sort_values(ascending=False)[:20].sort_index()\n",
    "pol_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_fp.footprints[\"MHD\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_fp.footprints[\"MHD\"].data[[\"wind_from_direction\", \"wind_speed\", \"atmosphere_boundary_layer_thickness\"]].sel(time=pol_events.index).compute().to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in multi_obs.obs[\"MHD\"].metadata.items() if \"station\" in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "!# in a notebook cell\n",
    "!wget -q -O natural_earth_50.zip \"https://naturalearth.s3.amazonaws.com/50m_cultural/ne_50m_admin_0_countries.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "world = gpd.read_file(\"natural_earth_50.zip\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(pol_events) // 2\n",
    "fig, axs = plt.subplots(nrows, 2, figsize=(15, 5 * nrows))\n",
    "times = [\"2015-04-07 15:00:00\", \"2015-11-01 06:00:00\", \"2015-02-11 11:00:00\", \"2015-08-17 09:00:00\", \"2015-03-19 09:00:00\", \"2015-10-04 04:00:00\"]\n",
    "times.sort()\n",
    "times = pol_events.index\n",
    "lat_min, lat_max = 40, 63\n",
    "lon_min, lon_max = -20, 15\n",
    "for time, ax, pe in zip(times, axs.flat, pol_events.values):\n",
    "    np.pow(multi_fp.footprints[\"MHD\"].data.fp.sel(time=time, lat=slice(lat_min, lat_max), lon=slice(lon_min, lon_max)), 0.1).plot(ax=ax, vmin=0, vmax=1.1)\n",
    "    world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "    ax.set_xlim(float(lon_min), float(lon_max))\n",
    "    ax.set_ylim(float(lat_min), float(lat_max))\n",
    "    ax.set_title(f\"time = {time}, pe={pe:.4f}ppt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_2010 = pd.read_csv(\"sf6_model_testing_data/sf6_info_2010.csv\")\n",
    "info_2015 = pd.read_csv(\"sf6_model_testing_data/sf6_info_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_2010.loc[30, \"longitude\"] *= -1  # fix glasgow longitude\n",
    "info_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_point_sources = gpd.GeoDataFrame({\"label\": info_2010.company_name.values}, geometry=gpd.points_from_xy(info_2010.longitude, info_2010.latitude), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "lon_min, lon_max = -15, 25\n",
    "lat_min, lat_max = 35, 65\n",
    "np.pow(flux_sel.flux, 0.1).plot(ax=ax)\n",
    "world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')\n",
    "# possible_point_sources.plot(ax=ax, color=\"red\", markersize=50, zorder=6)\n",
    "\n",
    "# Plot points (geometry.x = lon, geometry.y = lat)\n",
    "xs = possible_point_sources.geometry.x\n",
    "ys = possible_point_sources.geometry.y\n",
    "ax.scatter(xs, ys, s=40, c='red', edgecolor='k', zorder=6)\n",
    "\n",
    "# Add labels next to the points\n",
    "#labels = possible_point_sources['label']\n",
    "labels = info_2010.index\n",
    "for x, y, lab in zip(xs, ys, labels):\n",
    "    # ax.text(x + 0.05, y + 0.05, lab, fontsize=8, zorder=7, color=\"white\")  # tweak offsets as needed\n",
    "    ax.annotate(\n",
    "        lab,\n",
    "        xy=(x, y),                    # data coords for the point\n",
    "        xytext=(3, 3),                # offset in points (x, y)\n",
    "        textcoords='offset points',   # interpret xytext in display points\n",
    "        fontsize=8,\n",
    "        zorder=7,\n",
    "        bbox=dict(facecolor='white', alpha=0.6, pad=1),\n",
    "        clip_on=True,\n",
    "    )\n",
    "\n",
    "ax.set_xlim(float(lon_min), float(lon_max))\n",
    "ax.set_ylim(float(lat_min), float(lat_max))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "## Pipeline for making synth obs\n",
    "\n",
    "1. pollution events: load edgar flux, load multi_fp, map over data tree\n",
    "2. get baseline: load merged data, compute NESW and sum over regions; fill this to match obs\n",
    "3. get dictionaries of data sets for PE and baseline (not sure how to combine DataTrees)\n",
    "4. make synth obs\n",
    "5. make noise: \"winsorised\" stdev of real obs minus baseline (to de-trend); take the median and use the same value for all times as sigma; scale this median (0.9 seems \"about right\", could do less for \"easier\" scenarios)\n",
    "6. make `sf6` and `sf6_repeatability`; repeatability will just be the sigma used to make the noise (?)\n",
    "7. standardise\n",
    "   - get metadata from obs: species, site, inlet\n",
    "   - use source_format = \"openghg\"\n",
    "   - sampling period can be 1h? ...no this is e.g. 20 minutes for Medusa\n",
    "   - add calibration scale?\n",
    "   - use e.g. instrument: \"edgar-v8_mod-baseline_sigma-0.9\" and \"dataset_source\": \"synthetic\"\n",
    "   - info_metadata = {\"flux\": \"edgar-annual-total\", \"baseline\": \"modelled baseline\", \"noise\": \"normal\", \"noise_scaling\": 0.9}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "### How should we standardise?\n",
    "\n",
    "Use instrument to encode some features, put details on the features into \"additional metadata\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(multi_obs.obs[\"MHD\"].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.standardise import standardise_surface\n",
    "standardise_surface?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "And the schema we need to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.store import ObsSurface\n",
    "ObsSurface.schema(\"sf6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.types import MetadataAndData\n",
    "\n",
    "bucket = get_readable_buckets()[\"sf6_testing_store\"]\n",
    "ObsSurface(bucket=bucket).get_lookup_keys([MetadataAndData(metadata={}, data=synth_obs[\"MHD\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsSurface(bucket=bucket).add_metakeys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### Collecting data creation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multi_obs(params: dict, **kwargs) -> MultiObs:\n",
    "    params = params | kwargs\n",
    "    obs_params, _ = split_function_inputs(params, MultiObs.__init__)\n",
    "    obs_params[\"inlets\"] = params[\"inlet\"]\n",
    "    obs_params[\"instruments\"] = params[\"instrument\"]\n",
    "    obs_params[\"averaging_periods\"] = [\"1h\"] * len(obs_params[\"sites\"]) # TODO: don't hard code this\n",
    "    obs_params[\"obs_data_levels\"] = obs_params.get(\"obs_data_level\") or [None] * len(obs_params[\"sites\"])\n",
    "    return MultiObs(**obs_params)\n",
    "\n",
    "\n",
    "def get_multi_fp(params: dict, multi_obs: MultiObs | None = None, **kwargs) -> MultiFootprint:\n",
    "    params = params | kwargs\n",
    "    fp_params, _ = split_function_inputs(params, MultiFootprint.__init__)\n",
    "    fp_params[\"fp_heights\"] = params[\"fp_height\"]\n",
    "    fp_params[\"met_model\"] = [None] * len(fp_params[\"sites\"])\n",
    "    fp_params[\"model\"] = \"name\"\n",
    "\n",
    "    if multi_obs is None:\n",
    "        return MultiFootprint(**fp_params)\n",
    "\n",
    "    return MultiFootprint(**fp_params, obs_data=multi_obs.obs, obs_sites=multi_obs.sites)\n",
    "\n",
    "\n",
    "def flux_mult(footprint: xr.Dataset, flux: xr.Dataset, ffill: bool = False) -> xr.Dataset:\n",
    "    \"\"\"Map over a DataTree of footprints to make modelled pollution events.\"\"\"\n",
    "    if \"fp\" not in footprint.data_vars:\n",
    "        return footprint\n",
    "\n",
    "    if flux.sizes.get(\"time\") == 1:\n",
    "        flux = flux.squeeze(\"time\")\n",
    "        ffill = False\n",
    "    \n",
    "    if ffill:\n",
    "        flux = flux.reindex_like(footprint, method=\"ffill\")\n",
    "    result = footprint.copy()\n",
    "\n",
    "    fp_x_flux = (footprint.fp.pint.quantify() * flux.flux.pint.quantify()).pint.dequantify()\n",
    "    result[\"mod_obs\"] = fp_x_flux.sum([\"lat\", \"lon\"]).astype(\"float32\")\n",
    "    result[\"mod_obs\"].attrs[\"units\"] = fp_x_flux.attrs[\"units\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def make_mod_obs_dict_and_multi_obs_fp(ini_file: str | Path, flux: xr.Dataset, **kwargs):\n",
    "    \"\"\"Make dict of modelled obs datasets.\"\"\"\n",
    "    params = read_ini(ini_file)\n",
    "    multi_obs = get_multi_obs(params, **kwargs)\n",
    "    multi_fp = get_multi_fp(params, multi_obs, **kwargs)\n",
    "\n",
    "    fp_dt = xr.DataTree.from_dict({k: v.data for k, v in multi_fp.footprints.items()}).map_over_datasets(flux_mult, flux, True)\n",
    "\n",
    "    def get_mod_obs(ds: xr.Dataset) -> xr.Dataset:\n",
    "        if \"mod_obs\" not in ds.data_vars:\n",
    "            return ds\n",
    "        return ds[[\"mod_obs\"]].pint.quantify().pint.to(\"ppt\").pint.dequantify().compute()\n",
    "\n",
    "    all_mod_obs = fp_dt.map_over_datasets(get_mod_obs)\n",
    "    mod_obs_dict = {k: v.to_dataset() for k, v in all_mod_obs.items()}\n",
    "    return mod_obs_dict, multi_obs, multi_fp\n",
    "\n",
    "\n",
    "def make_baseline_dict(merged_data: xr.DataTree) -> dict[str, xr.Dataset]:\n",
    "    def make_prior_baseline(ds: xr.Dataset) -> xr.Dataset:\n",
    "        if \"bc_n\" not in ds.data_vars:\n",
    "            return ds\n",
    "        return nesw_bc_basis(ds).sum(\"bc_region\").rename(\"baseline\").to_dataset()\n",
    "\n",
    "    baselines = merged_data.scenario.map_over_datasets(make_prior_baseline)    \n",
    "    return {k: v.to_dataset() for k, v in baselines.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_key(k: str) -> bool:\n",
    "    if k in (\"site\", \"species\", \"units\", \"inlet\"):\n",
    "        return True\n",
    "    if any(x in k for x in (\"inlet\", \"station\", \"sampling\", \"data_owner\")):\n",
    "        return True\n",
    "    return False\n",
    "                \n",
    "def make_synth_obs(\n",
    "    mod_obs_dict: dict,\n",
    "    baseline_dict: dict,\n",
    "    multi_obs: MultiObs, \n",
    "    std_cutoff: float = 0.7, \n",
    "    std_resample: str = \"8D\", \n",
    "    std_scaling: float = 0.9,\n",
    "    seed: int | None = 123456789,\n",
    "    bias: float | dict[str, float] | None = None\n",
    ") -> dict[str, xr.Dataset]:\n",
    "    synth_obs = {}\n",
    "\n",
    "    if bias is None:\n",
    "        bias = 0.0\n",
    "\n",
    "    if isinstance(bias, float):\n",
    "        bias = {site: bias for site in mod_obs_dict}\n",
    "    \n",
    "    for site, mod_obs in mod_obs_dict.items():\n",
    "        baseline = baseline_dict[site]\n",
    "        baseline = baseline.reindex_like(mod_obs, method=\"nearest\")\n",
    "        ds = xr.merge([mod_obs, baseline])\n",
    "\n",
    "        obs = multi_obs.obs[site.upper()].data\n",
    "        species = obs.attrs.get(\"species\") or multi_obs[site.upper()].metadata.get(\"species\")\n",
    "        species = species.lower()\n",
    "\n",
    "        \n",
    "        std = std_scaling * ((obs.mf\n",
    "            .where(obs.mf < obs.mf.quantile(std_cutoff).values, drop=True)\n",
    "            .resample(time=std_resample).std())\n",
    "    #        .reindex_like(mod_obs.mod_obs, method=\"nearest\")\n",
    "            .median()\n",
    "            )\n",
    "        ds[\"noise\"] = make_noise_like(mod_obs.mod_obs, sigma=std.values, seed=seed)\n",
    "        try:\n",
    "            ds[\"noise\"].attrs[\"sigma\"] = float(std.values)\n",
    "        except (AttributeError, ValueError):\n",
    "            continue\n",
    "\n",
    "        if std_scaling != 0.0:\n",
    "            ds[species] = mod_obs.mod_obs + baseline.baseline + bias.get(site, 0.0) + ds[\"noise\"]\n",
    "        else:\n",
    "            ds[species] = mod_obs.mod_obs + baseline.baseline + bias.get(site, 0.0)\n",
    "            \n",
    "        ds[species].attrs = obs.mf.attrs\n",
    "\n",
    "                \n",
    "        global_attrs = {k: v for k, v in obs.attrs.items() if choose_key(k)}\n",
    "\n",
    "        # we used 1h averaging, so set sampling period...\n",
    "        global_attrs[\"sampling_period\"] = \"3600.0\"\n",
    "        global_attrs[\"sampling_period_unit\"] = \"s\"\n",
    "        \n",
    "        ds.attrs = global_attrs\n",
    "        \n",
    "        ds[f\"{species}_repeatability\"] = std * xr.ones_like(ds[species])\n",
    "        ds[f\"{species}_repeatability\"].attrs[\"units\"] = ds[species].attrs[\"units\"]\n",
    "        synth_obs[site] = ds.compute()\n",
    "\n",
    "    return synth_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "standardise\n",
    "get metadata from obs: species, site, inlet\n",
    "use source_format = \"openghg\"\n",
    "sampling period can be 1h? ...no this is e.g. 20 minutes for Medusa\n",
    "add calibration scale?\n",
    "use e.g. instrument: \"edgar-v8_mod-baseline_sigma-0.9\" and \"dataset_source\": \"synthetic\"\n",
    "info_metadata = {\"flux\": \"edgar-annual-total\", \"baseline\": \"modelled baseline\", \"noise\": \"normal\", \"noise_scaling\": 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def make_standardise_args(sites: list[str], multi_obs: MultiObs, instrument: str, flux: FluxData | None = None, **kwargs) -> dict[str, dict]:\n",
    "    \"\"\"For each site, create a dict of args to pass to standardise_surface.\"\"\"\n",
    "    result = defaultdict(dict)\n",
    "\n",
    "    for site in sites:\n",
    "        result[site][\"site\"] = site\n",
    "        result[site][\"source_format\"] = \"openghg\"\n",
    "        result[site][\"dataset_source\"] = \"synthetic\"\n",
    "        result[site][\"instrument\"] = instrument\n",
    "        result[site][\"info_metadata\"] = kwargs\n",
    "        result[site][\"update_mismatch\"] = \"metadata\"\n",
    "\n",
    "        \n",
    "        if site.upper() in multi_obs.obs:\n",
    "            attrs = multi_obs.obs[site.upper()].data.attrs\n",
    "            meta = multi_obs.obs[site.upper()].metadata\n",
    "#            result[site][\"species\"] = meta.get(\"species\")\n",
    "            result[site][\"inlet\"] = meta.get(\"inlet\")\n",
    "            result[site][\"network\"] = \"synthetic\" # meta.get(\"network\") or attrs.get(\"network\", \"synthetic\")\n",
    "            result[site][\"calibration_scale\"] = meta.get(\"scale\") or meta.get(\"calibration_scale\", \"synthetic\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "#### Step 1: get flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_obj = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\")\n",
    "flux = flux_obj.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "#### Step 2: get mod obs dict and multi obs/fp\n",
    "\n",
    "- Get ini file\n",
    "- pass start and end date as kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_obs_dict, multi_obs, multi_fp = make_mod_obs_dict_and_multi_obs_fp(ini_files[0], flux, start_date=\"2015-01-01\", end_date=\"2025-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "#### Step 3: make baselines\n",
    "\n",
    "We need to load all merged data, make baseline dicts for each year, then concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"sf6_model_testing_data/\")\n",
    "search_merged_data(data_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged_data = [load_merged_data(data_path, start_date=start_date, species=\"sf6\") for start_date in search_merged_data(data_path).start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dicts = [make_baseline_dict(merged_data) for merged_data in all_merged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "all_sites = set(chain.from_iterable([tuple(bld.keys()) for bld in baseline_dicts]))\n",
    "all_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_baseline_dict = {}\n",
    "\n",
    "for site in all_sites:\n",
    "    data = []\n",
    "    for baseline_dict in baseline_dicts:\n",
    "        if site in baseline_dict:\n",
    "            data.append(baseline_dict[site])\n",
    "    combined_baseline_dict[site] = xr.concat(data, dim=\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_baseline_dict[\"MHD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "#### Step 4: make synth obs\n",
    "\n",
    "Combine mod obs and baseline, plus add noise\n",
    "\n",
    "We can make multiple versions with different levels of noise, and versions with biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_obs_args1 = {   \n",
    "    \"std_cutoff\": 0.7, \n",
    "    \"std_resample\": \"8D\", \n",
    "    \"std_scaling\": 0.0,\n",
    "    \"seed\": 123456789,\n",
    "    \"bias\": None,\n",
    "}\n",
    "\n",
    "synth_obs1 = make_synth_obs(mod_obs_dict=mod_obs_dict, baseline_dict=combined_baseline_dict, multi_obs=multi_obs, **synth_obs_args1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_obs1[\"MHD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "#### Step 5: standardise\n",
    "\n",
    "we need to make args for standardise_surface, save to a temporary netcdf, then standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_args1 = make_standardise_args(sites=list(synth_obs1.keys()), multi_obs=multi_obs, instrument=\"edgar-annual-total_mod-baseline_no-noise_no-bias\", **synth_obs_args1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in std_args1[\"CBW\"].items():\n",
    "    print(k, v, type(v))\n",
    "    if isinstance(v, dict):\n",
    "        for k1, v2 in v.items():\n",
    "            print(\"\\t\", k1, type(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in synth_obs1[\"JFJ\"].attrs.items():\n",
    "    if not choose_key(k):\n",
    "        continue\n",
    "    print(k, v, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile() as f:\n",
    "    ds = synth_obs1[\"JFJ\"].copy()\n",
    "    ds.noise.attrs.pop(\"sigma\", None)\n",
    "    ds.to_netcdf(f, engine=\"h5netcdf\")\n",
    "    %debug standardise_surface(filepath=f.name, store=\"sf6_testing_store\", if_exists=\"new\", **std_args1[\"JFJ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "for site, ds in synth_obs1.items():\n",
    "    print(\"\\n\", site)\n",
    "    ds = ds.copy()\n",
    "    with tempfile.NamedTemporaryFile() as f:\n",
    "        ds.noise.attrs.pop(\"sigma\", None)\n",
    "        def maybe_str(x):\n",
    "            try:\n",
    "                return str(x)\n",
    "            except Exception:\n",
    "                return x\n",
    "        ds.attrs = {k: maybe_str(v) for k, v in ds.attrs.items() if choose_key(k)}\n",
    "        if \"sampling_period\" not in ds.attrs:\n",
    "            ds.attrs[\"sampling_period\"] = \"3600.0\"\n",
    "#        for key in std_args1[site]:\n",
    "#            ds.attrs.pop(key, None)\n",
    "        ds.to_netcdf(f, engine=\"h5netcdf\")\n",
    "        try:\n",
    "            standardise_surface(filepath=f.name, store=\"sf6_testing_store\", if_exists=\"new\", **std_args1[site])\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            print(\"Attrs:\\n\", ds.attrs)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "Check progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_res = search_surface(species=\"sf6\", store=\"sf6_testing_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_res.results[[\"site\", \"network\", \"latest_version\", \"object_store\", \"uuid\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.dataobjects import data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = data_manager(data_type=\"surface\", store=\"sf6_testing_store\", species=\"sf6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.delete_datasource?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_delete = [k for k, v in dm.metadata.items() if v[\"network\"] != \"synthetic\"]\n",
    "for k in to_delete:\n",
    "    dm.delete_datasource(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "### Making more variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_str(x):\n",
    "    try:\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "\n",
    "def standardise_synth_obs(synth_obs: dict, std_args: dict, **kwargs):\n",
    "    for site, ds in synth_obs.items():\n",
    "        print(\"\\nStandardising:\", site)\n",
    "        ds = ds.copy()\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            ds.noise.attrs.pop(\"sigma\", None)\n",
    "\n",
    "            ds.attrs = {k: maybe_str(v) for k, v in ds.attrs.items() if choose_key(k)}\n",
    "            if \"sampling_period\" not in ds.attrs:\n",
    "                ds.attrs[\"sampling_period\"] = \"3600.0\"\n",
    "    \n",
    "            ds.to_netcdf(f, engine=\"h5netcdf\")\n",
    "            try:\n",
    "                standardise_surface(filepath=f.name, store=\"sf6_testing_store\", **std_args[site], **kwargs)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_obs_args2 = {   \n",
    "    \"std_cutoff\": 0.7, \n",
    "    \"std_resample\": \"8D\", \n",
    "    \"std_scaling\": 0.3,\n",
    "    \"seed\": 123456789,\n",
    "    \"bias\": None,\n",
    "}\n",
    "\n",
    "synth_obs2 = make_synth_obs(mod_obs_dict=mod_obs_dict, baseline_dict=combined_baseline_dict, multi_obs=multi_obs, **synth_obs_args2)\n",
    "std_args2 = make_standardise_args(sites=list(synth_obs2.keys()), multi_obs=multi_obs, instrument=\"edgar-annual-total_mod-baseline_0.3-noise_no-bias\", **synth_obs_args2)\n",
    "\n",
    "standardise_synth_obs(synth_obs2, std_args2)\n",
    "\n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_res = search_surface(species=\"sf6\", store=\"sf6_testing_store\")\n",
    "obs_res.results.instrument.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scale in [0.6]: #[0.0, 0.3, 0.9, 1.2, 2.0]:\n",
    "    for bias in [None, -0.03, -0.1, 0.03, 0.1]:\n",
    "        synth_obs_args = {   \n",
    "            \"std_cutoff\": 0.7, \n",
    "            \"std_resample\": \"8D\", \n",
    "            \"std_scaling\": scale,\n",
    "            \"seed\": 123456789,\n",
    "            \"bias\": bias,\n",
    "        }\n",
    "        if bias is not None and scale != 0.0:\n",
    "            instrument = f\"edgar-annual-total_mod-baseline_{scale:.1f}-noise_{bias:.2f}-bias\"\n",
    "        elif bias is not None:\n",
    "            instrument = f\"edgar-annual-total_mod-baseline_no-noise_{bias:.2f}-bias\"\n",
    "        elif scale != 0.0:\n",
    "            instrument = f\"edgar-annual-total_mod-baseline_{scale:.1f}-noise_no-bias\"\n",
    "        else:\n",
    "            instrument = f\"edgar-annual-total_mod-baseline_no-noise_no-bias\"\n",
    "\n",
    "        synth_obs = make_synth_obs(mod_obs_dict=mod_obs_dict, baseline_dict=combined_baseline_dict, multi_obs=multi_obs, **synth_obs_args)\n",
    "        std_args = make_standardise_args(sites=list(synth_obs.keys()), multi_obs=multi_obs, instrument=instrument, **synth_obs_args)\n",
    "        standardise_synth_obs(synth_obs, std_args, if_exists=\"new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = obs_res.results.loc[(obs_res.results.timestamp < \"2025-10-10 19:33:49\")].uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhd_obs = get_obs_surface(species=\"sf6\", store=\"sf6_testing_store\", site=\"mhd\", instrument=\"edgar-annual-total_mod-baseline_06-noise_no-bias\")\n",
    "mhd_obs_no_noise = get_obs_surface(species=\"sf6\", store=\"sf6_testing_store\", site=\"mhd\", instrument=\"edgar-annual-total_mod-baseline_no-noise_no-bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "mhd_obs.data.mf.sel(time=slice(\"2018-01-01\", \"2018-02-01\")).plot(ax=ax, label=\"synth\",alpha=0.4)\n",
    "#mhd_obs_no_noise.data.mf.sel(time=slice(\"2018-01-01\", \"2018-02-01\")).plot(ax=ax, label=\"synth, no noise\",alpha=0.4)\n",
    "multi_obs.obs[\"MHD\"].data.mf.sel(time=slice(\"2018-01-01\", \"2018-02-01\")).plot(ax=ax, label=\"true\", alpha=0.4)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "# Storing more fluxes, baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_results_path = Path(\"/group/chem/acrg/PARIS_results_sharing/sf6_for_brendan/\")\n",
    "!ls -ls {sf6_results_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_res_files = !ls {sf6_results_path} | grep -v \"concentrations\"\n",
    "sf6_res_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_no = -2\n",
    "ds = xr.open_dataset(sf6_results_path / sf6_res_files[file_no], engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.country.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.country_flux_total_posterior.sel(country=[\"CHE\", \"DEU\"]).to_series().unstack().iloc[-10:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "year = 2018\n",
    "fig.suptitle(sf6_res_files[file_no][:-3] + f\" {year}\")\n",
    "\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "#vmin, vmax = -39, -26\n",
    "vmin, vmax = 0, 2.5e-12\n",
    "\n",
    "(ds.flux_total_prior).sel(time=f\"{year}-07-01\", method=\"nearest\").sel(latitude=lat_slice, longitude=lon_slice).plot(ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(\"prior\")\n",
    "\n",
    "(ds.flux_total_posterior).sel(time=f\"{year}-07-01\", method=\"nearest\").sel(latitude=lat_slice, longitude=lon_slice).plot(ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(\"posterior\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_rhime_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/\")\n",
    "!ls {sf6_rhime_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {sf6_rhime_path / \"RHIME_NAME_EUROPE_FLAT_PARISNID2026_sf6_yearly\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "ds2 = xr.open_dataset(sf6_rhime_path / \"RHIME_NAME_EUROPE_FLAT_PARISNID2026_sf6_yearly\" / f\"SF6_EUROPE_PARIS_conc_{year}-01-01.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.sitenames.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.retrieve import *\n",
    "\n",
    "obs_res = search_surface(species=\"sf6\", site=\"KIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs_res.retrieve_all()\n",
    "obs[1].data.sf6.time[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = xr.open_mfdataset(str(sf6_rhime_path / \"RHIME_NAME_EUROPE_FLAT_PARISNID2026_sf6_yearly\" / \"SF6_EUROPE_PARIS_flux_*-01-01.nc\"))\n",
    "print(ds3.country_flux_total_posterior.sel(country=[\"CHE\", \"DEU\"]).to_series().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3.country_flux_total_posterior.sel(country=[c for c in ds3.country.values if len(c) == 3]).to_series().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "fig.suptitle(f\"RHIME_NAME_EUROPE_FLAT_PARISNID2026_sf6_yearly {year}\")\n",
    "\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "#vmin, vmax = -39, -26\n",
    "vmin, vmax = 0, 10e-13\n",
    "\n",
    "(ds2.flux_total_prior).sel(latitude=lat_slice, longitude=lon_slice).plot(ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "axs[0].set_title(\"prior\")\n",
    "\n",
    "(ds2.flux_total_posterior).sel(latitude=lat_slice, longitude=lon_slice).plot(ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "axs[1].set_title(\"posterior\")\n",
    "\n",
    "for ax in axs.flat:\n",
    "    world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {},
   "source": [
    "## Baselines from InTEM and ELRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_conc_files = !ls {sf6_results_path} | grep \"concentrations\"\n",
    "sf6_conc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc1 = xr.open_dataset(sf6_results_path / sf6_conc_files[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_flux_index_coord(ds):\n",
    "    mindex = pd.MultiIndex.from_arrays([ds.platform.values[ds.number_of_identifier.values.astype(int)], ds.time.values], names=[\"platform\", \"time\"])\n",
    "    ds = ds.assign_coords(xr.Coordinates.from_pandas_multiindex(mindex, \"index\"))\n",
    "    return ds\n",
    "\n",
    "conc1 = fix_flux_index_coord(conc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc1.stdev_mf_total.sel(platform=\"MHD\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
