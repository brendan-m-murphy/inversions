{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Define a simple model or two and see how well they work.\n",
    "\n",
    "Our current model is quite complicated, so it might be helpful to see if a simple model works at all.\n",
    "\n",
    "Also, the simple models will parameters that are easier to interpret, so tuning uncertainties should be somewhat easier.\n",
    "\n",
    "## Simple model 1: Gaussian\n",
    "\n",
    "A fully Gaussian model would permit a \"classical\" analysis of the error terms.\n",
    "\n",
    "This model might also allow us to explore adding correlations while taking advantage of faster methods for fitting the model.\n",
    "\n",
    "Initially, using PyMC and MCMC will let us get started quickly.\n",
    "\n",
    "## Simple model 2: MCMC with lognormal prior, Gaussian likelihood\n",
    "\n",
    "We can add a prior on the variance of the Gaussian likelihood, but we won't add any scaling for pollution events.\n",
    "\n",
    "## Simple model 3(?): Gaussian with log transformed flux\n",
    "\n",
    "This is a simple way to enforce non-negative flux scaling factors.\n",
    "\n",
    "# Set-up\n",
    "\n",
    "## Data\n",
    "\n",
    "We're going to use Helene's SF6 data config for 2015-2019, since the model has problems here and there is no flask data to complicate matters.\n",
    "\n",
    "Caching the combined data will be useful, but we need to decide when the basis functions will be applied.\n",
    "To start with, maybe we should just use the same set-up as Helene (250 basis functions, weighted).\n",
    "\n",
    "## Modelling and Sampling\n",
    "\n",
    "- Make the models using some of the components from `likelihood_tests.py`\n",
    "- Using NUTS for 1000-2000 samples should be sufficient for testing.\n",
    "- Variational inference might be faster but I've never used it\n",
    "\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- If we use PARIS post-processing, we'll be able to use the code from the first notebook to work with fluxy, although we might need to extract some scripts.\n",
    "- Saving the the inversion output object will help with inspection (although we can just retrieve it in memory in this notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We'll use the script from `likelihood_tests.py` then decide how to save the data.\n",
    "\n",
    "It would be nice to parallelise this...\n",
    "\n",
    "First let's find the ini files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sf6_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/\")\n",
    "sf6_base_nid2025_path = sf6_path / \"RHIME_NAME_EUROPE_FLAT_ConfigNID2025_sf6_yearly\"\n",
    "ini_files = !ls {sf6_base_nid2025_path / \"*.ini\"}\n",
    "# get 2015-2019\n",
    "ini_files = ini_files[2:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run likelihood_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.bag as db\n",
    "\n",
    "data_dicts = db.from_sequence(ini_files).map(get_fp_data_dict).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The best way to save this data is probably using xr.DataTree, but this isn't how we currently save \"combined scenarios\".\n",
    "\n",
    "Let's try converting a \"fp_all\" dict to a DataTree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_all = data_dicts[0]\n",
    "print(fp_all.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "- \".species\", \".scales\", and \".units\" are attributes\n",
    "- all others are Datasets\n",
    "- Should \"MHD\", \"TAC\", etc. be in a subgroup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = {k: v for k, v in fp_all.items() if not k.startswith(\".\")}\n",
    "attrs = {\n",
    "    k.removeprefix(\".\"): v\n",
    "    for k, v in fp_all.items()\n",
    "    if k in [\".species\", \".scales\", \".units\"]\n",
    "}\n",
    "aux_data = {\n",
    "    k.removeprefix(\".\"): v\n",
    "    for k, v in fp_all.items()\n",
    "    if (k not in scenario) and (k.removeprefix(\".\") not in attrs)\n",
    "}\n",
    "\n",
    "# nest flux (this can be done automatically from nested dict according to xarray docs, but\n",
    "# it doesn't work for me... maybe I need to update xarray\n",
    "# aux_data[\"/flux\"] = xr.DataTree.from_dict({k: v.data for k, v in aux_data[\"flux\"].items()})\n",
    "# del aux_data[\"flux\"]\n",
    "\n",
    "# \"flux\" as a dataset... this might not work if we're mixing high/low frequency fluxes\n",
    "# but it works for multiple sectors\n",
    "aux_data[\"flux\"] = xr.Dataset({k: v.data.flux for k, v in aux_data[\"flux\"].items()})\n",
    "\n",
    "# get data from BoundaryConditionsData object... maybe we\n",
    "# should put metadata in global attrs for this group?\n",
    "aux_data[\"bc\"] = aux_data[\"bc\"].data\n",
    "\n",
    "# add basis as data variable at root?\n",
    "basis = aux_data[\"basis\"]\n",
    "del aux_data[\"basis\"]\n",
    "\n",
    "# add basis within group?\n",
    "# aux_data[\"basis\"] = aux_data[\"basis\"].rename(\"basis\").to_dataset()\n",
    "dt_dict = aux_data.copy()\n",
    "dt_dict[\"/scenario\"] = xr.DataTree.from_dict({k: v for k, v in scenario.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = xr.DataTree.from_dict(dt_dict)\n",
    "dt.attrs = attrs\n",
    "dt[\"basis\"] = basis\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "It might be a bit of a pain to round-trip this to the original structure, but this should work fine for storing the data, and restoring the \"scenario\" part will be easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dt.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Okay, that seems pretty easy to revert to the \"fp_all\" format.\n",
    "\n",
    "Let's make a function to create a DataTree from the \"fp_all\" style dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_all_to_datatree(fp_all: dict, name: str | None = None) -> xr.DataTree:\n",
    "    scenario = {k: v for k, v in fp_all.items() if not k.startswith(\".\")}\n",
    "    attrs = {\n",
    "        k.removeprefix(\".\"): v\n",
    "        for k, v in fp_all.items()\n",
    "        if k in [\".species\", \".scales\", \".units\"]\n",
    "    }\n",
    "    aux_data = {\n",
    "        k.removeprefix(\".\"): v\n",
    "        for k, v in fp_all.items()\n",
    "        if (k not in scenario) and (k.removeprefix(\".\") not in attrs)\n",
    "    }\n",
    "\n",
    "    # nest flux (this can be done automatically from nested dict according to xarray docs, but\n",
    "    # it doesn't work for me... maybe I need to update xarray\n",
    "    # aux_data[\"/flux\"] = xr.DataTree.from_dict({k: v.data for k, v in aux_data[\"flux\"].items()})\n",
    "    # del aux_data[\"flux\"]\n",
    "\n",
    "    # \"flux\" as a dataset... this might not work if we're mixing high/low frequency fluxes\n",
    "    # but it works for multiple sectors\n",
    "    aux_data[\"flux\"] = xr.Dataset({k: v.data.flux for k, v in aux_data[\"flux\"].items()})\n",
    "\n",
    "    # get data from BoundaryConditionsData object... maybe we\n",
    "    # should put metadata in global attrs for this group?\n",
    "    aux_data[\"bc\"] = aux_data[\"bc\"].data\n",
    "\n",
    "    # fix issue with units for time coord\n",
    "    if \"units\" in aux_data[\"bc\"].coords[\"time\"].attrs:\n",
    "        del aux_data[\"bc\"].coords[\"time\"].attrs[\"units\"]\n",
    "\n",
    "    # add basis as data variable at root?\n",
    "    basis = aux_data[\"basis\"]\n",
    "    del aux_data[\"basis\"]\n",
    "\n",
    "    # add basis within group?\n",
    "    # aux_data[\"basis\"] = aux_data[\"basis\"].rename(\"basis\").to_dataset()\n",
    "\n",
    "    dt_dict = aux_data.copy()\n",
    "    dt_dict[\"/scenario\"] = xr.DataTree.from_dict({k: v for k, v in scenario.items()})\n",
    "\n",
    "    dt = xr.DataTree.from_dict(dt_dict)\n",
    "    dt.attrs = attrs\n",
    "    dt[\"basis\"] = basis\n",
    "\n",
    "    if name is not None:\n",
    "        dt.name = name\n",
    "\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.dataobjects import BoundaryConditionsData, FluxData\n",
    "\n",
    "\n",
    "def datatree_to_fp_all(dt: xr.DataTree) -> dict:\n",
    "    d = dt.to_dict()\n",
    "    result = {}\n",
    "    result[\".flux\"] = {\n",
    "        dv: FluxData(data=d[\"/flux\"][[dv]], metadata={}) for dv in d[\"/flux\"].data_vars\n",
    "    }\n",
    "    result[\".bc\"] = BoundaryConditionsData(data=d[\"/bc\"], metadata={})\n",
    "    result[\".basis\"] = d[\"/\"].basis\n",
    "    result[\".species\"] = dt.attrs.get(\"species\")\n",
    "    result[\".units\"] = dt.attrs.get(\"units\")\n",
    "    result[\".scales\"] = dt.attrs.get(\"scales\")\n",
    "    for k, v in d.items():\n",
    "        if k.startswith(\"/scenario/\"):\n",
    "            site = k.split(\"/\")[-1]\n",
    "            result[site] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatree_to_fp_all(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(dt.time.dt.year.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir sf6_model_testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_path = Path(\"/user/work/bm13805/\")\n",
    "data_path = work_path / \"sf6_model_testing_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_and_paths = []\n",
    "for dd in data_dicts:\n",
    "    dt = fp_all_to_datatree(dd)\n",
    "    try:\n",
    "        year = str(dt.flux.time.dt.year.values[0])\n",
    "    except IndexError:\n",
    "        year = str(dt.flux.time.dt.year.values)\n",
    "    out_path = data_path / f\"sf6_combined_data_{year}.zarr\"\n",
    "    dt_and_paths.append((dt, out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt0, path0 = dt_and_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunk_ds(ds: xr.Dataset) -> xr.Dataset:\n",
    "    default_chunks = {\"lat\": 293, \"lon\": 391, \"height\": 20, \"bc_region\": 4}\n",
    "    chunks = {dim: default_chunks.get(dim) for dim in ds.dims if dim != \"time\"}\n",
    "    if ds.sizes.get(\"time\", 0) > 240:\n",
    "        chunks[\"time\"] = 240\n",
    "    elif \"time\" in ds.dims:\n",
    "        chunks[\"time\"] = ds.sizes[\"time\"]\n",
    "    if \"region\" in ds.dims:\n",
    "        chunks[\"region\"] = ds.sizes[\"region\"]\n",
    "    return ds.chunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt0 = dt0.map_over_datasets(rechunk_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt0.to_zarr(path0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dt, path in dt_and_paths[1:]:\n",
    "#    dt.map_over_datasets(rechunk_ds).to_zarr(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = !ls {data_path} | grep combined_data\n",
    "data_paths = [data_path / f for f in data_files]\n",
    "data_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Now let's try reloaded the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inversions_experimental_code/data_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_all_2015 = datatree_to_fp_all(xr.open_datatree(data_paths[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_all_2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Preparing inversion inputs\n",
    "\n",
    "We'll use the `make_inv_inputs` function from `likelihood_tests.py`. This requires the \"fp_all\" (or \"fp_data\") dict, along with some parameters from the ini file. We'll get these first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "InversionInfo = namedtuple(\"InversionInfo\", \"fp_data,params\")\n",
    "\n",
    "inversion_info = {}\n",
    "for ini, dpath in zip(ini_files, data_paths):\n",
    "    params = read_ini(ini)\n",
    "    dt = xr.open_datatree(dpath)\n",
    "    try:\n",
    "        year = str(dt.flux.time.dt.year.values[0])\n",
    "    except IndexError:\n",
    "        year = str(dt.flux.time.dt.year.values)\n",
    "    fp_all = datatree_to_fp_all(dt)\n",
    "\n",
    "    inversion_info[year] = InversionInfo(fp_all, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_info[\"2015\"].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "InversionInput = namedtuple(\"InversionInput\", \"inv_input,params\")\n",
    "\n",
    "inversion_inputs = {}\n",
    "for k, v in inversion_info.items():\n",
    "    inv_input = make_inv_inputs(\n",
    "        v.fp_data,\n",
    "        bc_freq=v.params.get(\"bc_freq\"),\n",
    "        sigma_freq=v.params.get(\"sigma_freq\"),\n",
    "        min_error=v.params.get(\"min_error\") or v.params.get(\"calculate_min_error\"),\n",
    "    )\n",
    "    inversion_inputs[k] = InversionInput(inv_input, v.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_inputs[\"2015\"].inv_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# Model 2: Lognormal with Gaussian likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytensor before pymc so we can set config values\n",
    "import pytensor\n",
    "\n",
    "pytensor.config.floatX = \"float32\"\n",
    "pytensor.config.warn_float64 = \"warn\"\n",
    "\n",
    "import arviz as az\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run likelihood_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "inv_input_obj = inversion_inputs[\"2015\"]\n",
    "inv_input = inv_input_obj.inv_input\n",
    "params = inv_input_obj.params\n",
    "\n",
    "with pm.Model() as model:\n",
    "    mu = add_linear_component(\n",
    "        inv_input.H,\n",
    "        data_name=\"hx\",\n",
    "        prior_args=params[\"xprior\"],\n",
    "        var_name=\"x\",\n",
    "        output_name=\"mu\",\n",
    "    )\n",
    "    mu_bc = add_linear_component(\n",
    "        inv_input.H_bc,\n",
    "        data_name=\"hbc\",\n",
    "        prior_args=params[\"bcprior\"],\n",
    "        var_name=\"bc\",\n",
    "        output_name=\"mu_bc\",\n",
    "        compute_deterministic=True,\n",
    "    )\n",
    "\n",
    "    make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "    # make likelihood\n",
    "    Y = add_model_data(inv_input.mf, \"Y\")\n",
    "    error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "    min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "    sigma = make_sigma(\n",
    "        inv_input.site_indicator,\n",
    "        {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5},\n",
    "        inv_input.sigma_freq_index,\n",
    "    )\n",
    "\n",
    "    epsilon = pm.Deterministic(\n",
    "        \"epsilon\", pt.sqrt(error**2 + min_error**2 + 0.01 * sigma**2), dims=\"nmeasure\"\n",
    "    )\n",
    "    pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = pm.sample_prior_predictive(draws=10000, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_preds = trace.prior_predictive.y.assign_coords(\n",
    "    nmeasure=inv_input.nmeasure\n",
    ").squeeze(\"chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_prior = trace.prior.mu_bc.mean([\"chain\", \"draw\"]).assign_coords(\n",
    "    nmeasure=inv_input.nmeasure\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = list(np.unique(inv_input.site))\n",
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(15, 15))\n",
    "for site, ax in zip(sites, axs.flat):\n",
    "    for i in range(10):\n",
    "        prior_preds.sel(site=site).isel(draw=slice(i, None, 10)).mean(\"draw\").plot(\n",
    "            ax=ax, label=\"prior pred\", alpha=0.05, color=\"blue\"\n",
    "        )\n",
    "    inv_input.mf.sel(site=site).plot(ax=ax, label=\"obs\", alpha=0.5, color=\"orange\")\n",
    "    bc_prior.sel(site=site).plot(ax=ax, label=\"bc prior\", alpha=0.5, color=\"green\")\n",
    "\n",
    "# fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "These prior predictives look reasonable. We're not modelling the big pollution events, and the baseline is too high in some cases.\n",
    "\n",
    "What if we change the prior uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params[\"xprior\"])\n",
    "print(params[\"bcprior\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model2:\n",
    "    mu = add_linear_component(\n",
    "        inv_input.H,\n",
    "        data_name=\"hx\",\n",
    "        prior_args={\"pdf\": \"lognormal\", \"stdev\": 1.0},\n",
    "        var_name=\"x\",\n",
    "        output_name=\"mu\",\n",
    "    )\n",
    "    mu_bc = add_linear_component(\n",
    "        inv_input.H_bc,\n",
    "        data_name=\"hbc\",\n",
    "        prior_args=params[\"bcprior\"],\n",
    "        var_name=\"bc\",\n",
    "        output_name=\"mu_bc\",\n",
    "        compute_deterministic=True,\n",
    "    )\n",
    "\n",
    "    make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "    # make likelihood\n",
    "    Y = add_model_data(inv_input.mf, \"Y\")\n",
    "    error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "    min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "    sigma = make_sigma(\n",
    "        inv_input.site_indicator,\n",
    "        {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5},\n",
    "        inv_input.sigma_freq_index,\n",
    "    )\n",
    "\n",
    "    epsilon = pm.Deterministic(\n",
    "        \"epsilon\", pt.sqrt(error**2 + min_error**2 + 0.01 * sigma**2), dims=\"nmeasure\"\n",
    "    )\n",
    "    pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace2 = pm.sample_prior_predictive(draws=1000, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use quantiles instead of plotting lots of traces...\n",
    "def plot_prior_preds(trace, inv_input, skip=20):\n",
    "    prior_preds = trace.prior_predictive.y.assign_coords(\n",
    "        nmeasure=inv_input.nmeasure\n",
    "    ).squeeze(\"chain\")\n",
    "    bc_prior = trace.prior.mu_bc.mean([\"chain\", \"draw\"]).assign_coords(\n",
    "        nmeasure=inv_input.nmeasure\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(15, 15))\n",
    "    for site, ax in zip(sites, axs.flat):\n",
    "        for i in range(skip):\n",
    "            prior_preds.sel(site=site).isel(draw=slice(i, None, skip)).mean(\n",
    "                \"draw\"\n",
    "            ).plot(ax=ax, label=\"prior pred\", alpha=0.1 / np.sqrt(skip), color=\"blue\")\n",
    "        inv_input.mf.sel(site=site).plot(ax=ax, label=\"obs\", alpha=0.5, color=\"orange\")\n",
    "        bc_prior.sel(site=site).plot(ax=ax, label=\"bc prior\", alpha=0.5, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_preds(trace2, inv_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_preds(trace, inv_input, skip=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model3:\n",
    "    mu = add_linear_component(\n",
    "        inv_input.H,\n",
    "        data_name=\"hx\",\n",
    "        prior_args=params[\"xprior\"],\n",
    "        var_name=\"x\",\n",
    "        output_name=\"mu\",\n",
    "    )\n",
    "    mu_bc = add_linear_component(\n",
    "        inv_input.H_bc,\n",
    "        data_name=\"hbc\",\n",
    "        prior_args=params[\"bcprior\"],\n",
    "        var_name=\"bc\",\n",
    "        output_name=\"mu_bc\",\n",
    "        compute_deterministic=True,\n",
    "    )\n",
    "\n",
    "    make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "    # make likelihood\n",
    "    Y = add_model_data(inv_input.mf, \"Y\")\n",
    "    error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "    min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "    #    sigma = make_sigma(inv_input.site_indicator, {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5}, inv_input.sigma_freq_index)\n",
    "\n",
    "    epsilon = pm.Deterministic(\n",
    "        \"epsilon\", pt.sqrt(error**2 + min_error**2), dims=\"nmeasure\"\n",
    "    )\n",
    "    pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace3 = pm.sample_prior_predictive(draws=5000, model=model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_preds(trace3, inv_input, skip=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model4:\n",
    "    mu = add_linear_component(\n",
    "        inv_input.H,\n",
    "        data_name=\"hx\",\n",
    "        prior_args={\"pdf\": \"lognormal\", \"stdev\": 1.0},\n",
    "        var_name=\"x\",\n",
    "        output_name=\"mu\",\n",
    "    )\n",
    "    mu_bc = add_linear_component(\n",
    "        inv_input.H_bc,\n",
    "        data_name=\"hbc\",\n",
    "        prior_args=params[\"bcprior\"],\n",
    "        var_name=\"bc\",\n",
    "        output_name=\"mu_bc\",\n",
    "        compute_deterministic=True,\n",
    "    )\n",
    "\n",
    "    make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "    # make likelihood\n",
    "    Y = add_model_data(inv_input.mf, \"Y\")\n",
    "    error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "    min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "    #    sigma = make_sigma(inv_input.site_indicator, {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5}, inv_input.sigma_freq_index)\n",
    "\n",
    "    epsilon = pm.Deterministic(\n",
    "        \"epsilon\", pt.sqrt(error**2 + min_error**2), dims=\"nmeasure\"\n",
    "    )\n",
    "    pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")\n",
    "\n",
    "    trace4 = pm.sample_prior_predictive(draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_preds(trace4, inv_input, skip=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model5:\n",
    "    mu = add_linear_component(\n",
    "        inv_input.H,\n",
    "        data_name=\"hx\",\n",
    "        prior_args={\"pdf\": \"exponential\"},\n",
    "        var_name=\"x\",\n",
    "        output_name=\"mu\",\n",
    "    )\n",
    "    mu_bc = add_linear_component(\n",
    "        inv_input.H_bc,\n",
    "        data_name=\"hbc\",\n",
    "        prior_args=params[\"bcprior\"],\n",
    "        var_name=\"bc\",\n",
    "        output_name=\"mu_bc\",\n",
    "        compute_deterministic=True,\n",
    "    )\n",
    "\n",
    "    make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "    # make likelihood\n",
    "    Y = add_model_data(inv_input.mf, \"Y\")\n",
    "    error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "    min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "    #    sigma = make_sigma(inv_input.site_indicator, {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5}, inv_input.sigma_freq_index)\n",
    "\n",
    "    epsilon = pm.Deterministic(\n",
    "        \"epsilon\", pt.sqrt(error**2 + min_error**2), dims=\"nmeasure\"\n",
    "    )\n",
    "    pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")\n",
    "\n",
    "    trace5 = pm.sample_prior_predictive(draws=5000)\n",
    "\n",
    "plot_prior_preds(trace5, inv_input, skip=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Record deterministic for mu and compare mean/quantiles for mu with different priors. This will isolate the effects of the prior uncertainty.\n",
    "- Set up plots with quantiles/error bars and compare multiple models on one plot (fluxy functions useful?)\n",
    "- Create a class to organise experiments: include description, model, trace\n",
    "- Compute Bayesian R2 scores for prior predictives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cluster setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf6_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/brendan_tests\")\n",
    "\n",
    "!ls {sf6_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = sf6_path / \"simple_model_logs\"\n",
    "log_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    processes=1,\n",
    "    cores=8,\n",
    "    memory=\"50GB\",\n",
    "    walltime=\"00:30:00\",\n",
    "    account=\"chem007981\",\n",
    "    log_directory=str(log_path),\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_workers(client, cluster):\n",
    "    available_workers = [\n",
    "        v.get(\"id\")\n",
    "        for v in client.scheduler_info(n_workers=len(cluster.workers))[\n",
    "            \"workers\"\n",
    "        ].values()\n",
    "    ]\n",
    "    return available_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_available_workers(client, cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "## Model/sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(inv_input):\n",
    "    with pm.Model() as model:\n",
    "        mu = add_linear_component(\n",
    "            inv_input.H,\n",
    "            data_name=\"hx\",\n",
    "            prior_args={\"pdf\": \"lognormal\", \"stdev\": 2.0},\n",
    "            var_name=\"x\",\n",
    "            output_name=\"mu\",\n",
    "        )\n",
    "        mu_bc = add_linear_component(\n",
    "            inv_input.H_bc,\n",
    "            data_name=\"hbc\",\n",
    "            prior_args={\"pdf\": \"truncatednormal\", \"mu\": 1.0, \"sigma\": 0.1},\n",
    "            var_name=\"bc\",\n",
    "            output_name=\"mu_bc\",\n",
    "            compute_deterministic=True,\n",
    "        )\n",
    "\n",
    "        mu_bc += make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "        # make likelihood\n",
    "        Y = add_model_data(inv_input.mf, \"Y\")\n",
    "        error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "        min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "        sigma = make_sigma(\n",
    "            inv_input.site_indicator,\n",
    "            {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5},\n",
    "            inv_input.sigma_freq_index,\n",
    "        )\n",
    "\n",
    "        epsilon = pm.Deterministic(\n",
    "            \"epsilon\",\n",
    "            pt.sqrt(error**2 + min_error**2 + 0.01 * sigma**2),\n",
    "            dims=\"nmeasure\",\n",
    "        )\n",
    "        pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_input_obj = inversion_inputs[\"2015\"]\n",
    "inv_input = inv_input_obj.inv_input\n",
    "params = inv_input_obj.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_kwargs = default_sample_kwargs.copy()\n",
    "sample_kwargs[\"blas_cores\"] = 8\n",
    "sample_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(inv_input)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample_prior_predictive(draws=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prior_preds(trace, inv_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "## Running on SLURM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = sf6_path / \"simple_models\" / \"model2\"\n",
    "out_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = \"\"\"\n",
    "def make_model(inv_input):\n",
    "    with pm.Model() as model:\n",
    "        mu = add_linear_component(\n",
    "            inv_input.H,\n",
    "            data_name=\"hx\",\n",
    "            prior_args={\"pdf\": \"lognormal\", \"stdev\": 2.0},\n",
    "            var_name=\"x\",\n",
    "            output_name=\"mu\",\n",
    "        )\n",
    "        mu_bc = add_linear_component(\n",
    "            inv_input.H_bc,\n",
    "            data_name=\"hbc\",\n",
    "            prior_args={\"pdf\": \"truncatednormal\", \"mu\": 1.0, \"sigma\": 0.1},\n",
    "            var_name=\"bc\",\n",
    "            output_name=\"mu_bc\",\n",
    "            compute_deterministic=True,\n",
    "        )\n",
    "\n",
    "        mu_bc += make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "    \n",
    "        # make likelihood\n",
    "        Y = add_model_data(inv_input.mf, \"Y\")\n",
    "        error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "        min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "        sigma = make_sigma(inv_input.site_indicator, {\"pdf\": \"inversegamma\", \"alpha\": 2.5, \"beta\": 5}, inv_input.sigma_freq_index)\n",
    "\n",
    "        epsilon = pm.Deterministic(\"epsilon\", pt.sqrt(error**2 + min_error**2 + 0.01 * sigma**2), dims=\"nmeasure\")\n",
    "        pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")\n",
    "\n",
    "    return model\n",
    "\"\"\"\n",
    "with open(out_path / \"model_code.txt\", \"wt\") as f:\n",
    "    f.write(model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inversion(inv_input, year, model_func=make_model, out_path=out_path, **kwargs):\n",
    "    out_file = out_path / f\"trace_{year}.nc\"\n",
    "\n",
    "    print(\"Staring sampling for year\", year)\n",
    "    with model_func(inv_input, **kwargs):\n",
    "        idata = pm.sample(**sample_kwargs)\n",
    "    print(f\"Sampling for year {year} complete.\")\n",
    "    print(f\"Writing idata for year {year}.\")\n",
    "    idata.to_netcdf(out_file)\n",
    "    print(f\"idata for year {year} saved to {out_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "futures = []\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    inv_input = inversion_inputs[str(year)].inv_input\n",
    "    func = partial(run_inversion, inv_input, year)\n",
    "    future = client.submit(func)\n",
    "    futures.append(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_futures(client):\n",
    "    return [Future(key, client) for key in client.refcount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "get_futures(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Hierarchical sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hierarchical_model(inv_input, sigma_prior: dict, sigma_hyper_prior: dict):\n",
    "    with pm.Model() as model:\n",
    "        mu = add_linear_component(\n",
    "            inv_input.H,\n",
    "            data_name=\"hx\",\n",
    "            prior_args={\"pdf\": \"lognormal\", \"stdev\": 2.0},\n",
    "            var_name=\"x\",\n",
    "            output_name=\"mu\",\n",
    "        )\n",
    "        mu_bc = add_linear_component(\n",
    "            inv_input.H_bc,\n",
    "            data_name=\"hbc\",\n",
    "            prior_args={\"pdf\": \"truncatednormal\", \"mu\": 1.0, \"sigma\": 0.1},\n",
    "            var_name=\"bc\",\n",
    "            output_name=\"mu_bc\",\n",
    "            compute_deterministic=True,\n",
    "        )\n",
    "\n",
    "        offset = make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "        mu_bc = mu_bc + offset\n",
    "\n",
    "        # make likelihood\n",
    "        Y = add_model_data(inv_input.mf, \"Y\")\n",
    "        error = add_model_data(inv_input.mf_error.astype(\"float32\"), \"error\")\n",
    "        min_error = add_model_data(inv_input.min_error.astype(\"float32\"), \"min_error\")\n",
    "\n",
    "        sigma_hyper = parse_prior(\"sigma_hyper\", sigma_hyper_prior)\n",
    "\n",
    "        sigma_prior = sigma_prior or {\"pdf\": \"halfnormal\"}\n",
    "        sigma0 = parse_prior(\"sigma0\", sigma_prior, dims=\"nmeasure\")\n",
    "        sigma = pm.Deterministic(\"sigma\", sigma_hyper * sigma0)\n",
    "\n",
    "        epsilon = pm.Deterministic(\n",
    "            \"epsilon\", pt.sqrt(error**2 + sigma**2 + min_error**2), dims=\"nmeasure\"\n",
    "        )\n",
    "        pm.Normal(\"y\", mu=mu + mu_bc, sigma=epsilon, observed=Y, dims=\"nmeasure\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_priors = dict(\n",
    "    sigma_hyper_prior={\"pdf\": \"inversegamma\", \"alpha\": 3, \"beta\": 2},\n",
    "    sigma_prior={\"pdf\": \"halfstudentt\", \"nu\": 2.0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmodel = make_hierarchical_model(\n",
    "    inv_input,\n",
    "    sigma_hyper_prior={\"pdf\": \"inversegamma\", \"alpha\": 3, \"beta\": 2},\n",
    "    sigma_prior={\"pdf\": \"halfnormal\"},\n",
    ")\n",
    "\n",
    "with hmodel:\n",
    "    htrace = pm.sample_prior_predictive(draws=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_prior_preds(htrace, inv_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "hout_path = sf6_path / \"simple_models\" / \"hierarchical_model\"\n",
    "hout_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for year in range(2015, 2020):\n",
    "    inv_input = inversion_inputs[str(year)].inv_input\n",
    "    key = f\"run_inversion-hierarchical_{year}\"\n",
    "    func = partial(\n",
    "        run_inversion,\n",
    "        inv_input=inv_input,\n",
    "        year=year,\n",
    "        model_func=make_hierarchical_model,\n",
    "        out_path=hout_path,\n",
    "        **sig_priors,\n",
    "    )\n",
    "    future = client.submit(func, key=key)\n",
    "    futures.append(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = [f for f in futures if f.status != \"cancelled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -R {out_path.parent.parent/\"brendan_tests\"/\"simple_models\"}\n",
    "# out_path = out_path.parent.parent/\"brendan_tests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "trace_files = defaultdict(list)\n",
    "\n",
    "\n",
    "for root, _, files in os.walk(out_path / \"simple_models\"):\n",
    "    for f in files:\n",
    "        if f.endswith(\"nc\"):\n",
    "            trace_files[root].append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "traces = defaultdict(list)\n",
    "\n",
    "for k, v in trace_files.items():\n",
    "    for f in v:\n",
    "        traces[Path(k).name].append(az.InferenceData.from_netcdf(Path(k) / f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum0 = performance_summary(traces[\"hierarchical_model\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_input0 = inversion_inputs[\"2018\"].inv_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmodel = make_hierarchical_model(inv_input0, **sig_priors)\n",
    "trace0 = traces[\"hierarchical_model\"][0]\n",
    "trace0.posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trace0[\"prior\"]\n",
    "# del trace0[\"prior_predictive\"]\n",
    "# del trace0[\"posterior_predictive\"]\n",
    "with hmodel:\n",
    "    trace0.extend(pm.sample_prior_predictive(draws=1000))\n",
    "    trace0.extend(pm.sample_posterior_predictive(trace=trace0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = inversion_info[\"2018\"].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.retrieve import *\n",
    "\n",
    "flux = get_flux(species=\"sf6\", domain=\"europe\", source=\"flat-annual-total\").data.flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_out = make_inv_out(inv_input0, trace0, flux, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg_inversions.postprocessing.countries import Countries\n",
    "\n",
    "default_country_file = Path(\n",
    "    \"/group/chem/acrg/LPDM/countries/country_EUROPE_EEZ_PARIS_gapfilled.nc\"\n",
    ")\n",
    "default_countries = [\"BEL\", \"NLD\", \"BENELUX\", \"DEU\", \"FRA\", \"GBR\", \"IRL\", \"NW_EU\"]\n",
    "\n",
    "\n",
    "countries = Countries.from_file(\n",
    "    country_file=default_country_file, country_code=\"alpha3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_out.trace = inv_out.trace.isel(chain=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg_inversions.postprocessing.make_paris_outputs import (\n",
    "    paris_concentration_outputs,\n",
    ")\n",
    "\n",
    "# conc, flux = make_paris_outputs(inv_out, default_country_file, time_point=\"start\", inversion_grid=False)\n",
    "conc = paris_concentration_outputs(inv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = conc.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(15, 22))\n",
    "\n",
    "for site, ax in zip(range(6), axs.flat):\n",
    "    co_sel = conc.isel(nsite=site)\n",
    "    co_sel.Yobs.plot(ax=ax, label=\"y obs\")\n",
    "    co_sel.Yapost.plot(ax=ax, label=\"a post\")\n",
    "    ax.fill_between(\n",
    "        co_sel.time.values,\n",
    "        co_sel.qYapost.isel(percentile=0).values,\n",
    "        co_sel.qYapost.isel(percentile=1).values,\n",
    "        alpha=0.5,\n",
    "        color=\"orange\",\n",
    "        interpolate=True,\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_title(conc.sitenames.values[site])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_df = countries.get_country_trace(inv_out).mean(\"draw\").to_series().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inversions_experimental_code/basis_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = BasisFunctions(inv_input0.basis_flat, flux.sel(time=[\"2018-01-01\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_post_mean = bf.interpolate(inv_out.trace.posterior.x.mean([\"draw\"]), flux=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "world = gpd.read_file(\"natural_earth_50.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "# vmin, vmax = -39, -26\n",
    "vmin, vmax = 0, 10e-13\n",
    "\n",
    "flux_post_mean.sel(lat=lat_slice, lon=lon_slice).plot(ax=ax)  # , vmin=vmin, vmax=vmax)\n",
    "\n",
    "world.boundary.plot(\n",
    "    ax=ax, linewidth=0.6, edgecolor=\"white\"\n",
    ")  # or .plot(facecolor='none')\n",
    "ax.set_xlim(lon_min, lon_max)\n",
    "ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
