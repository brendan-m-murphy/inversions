{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data for SF6 tests\n",
    "\n",
    "I already saved some data using 4h averaging and 250 basis functions, along with the filters we typically use for PARIS.\n",
    "\n",
    "It would be helpful to save the data in a state where I could change the number of basis functions or filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sf6_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/\")\n",
    "sf6_base_nid2025_path = sf6_path / \"RHIME_NAME_EUROPE_FLAT_ConfigNID2025_sf6_yearly\"\n",
    "ini_files = !ls {sf6_base_nid2025_path / \"*.ini\"}\n",
    "# get 2015-2024\n",
    "ini_files = ini_files[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inversions_experimental_code/data_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Test for MultiObs and MultiFootprint with ModelScenario\n",
    "\n",
    "Alignment failed when trying to pass MultiObs and MultiFootprint directly to ModelScenario, so I resorted to using a loop over sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg_inversions.hbmcmc.run_hbmcmc import hbmcmc_extract_param\n",
    "from openghg.util import split_function_inputs\n",
    "\n",
    "params = read_ini(ini_files[0])\n",
    "data_params, _ =  split_function_inputs(params, data_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_all = data_processing(**data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = fp_all_to_datatree(fp_all, rechunk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Storing this data for later processing\n",
    "\n",
    "If I want to store the data without applying basis functions or filtering, what do I need to store?\n",
    "- need mean fp x mean flux, but we can get this from fp x flux...\n",
    "- need some footprint info for filters (mostly met. data, only need `fp` for `local_influence` filter)\n",
    "\n",
    "What if we want to resample again?\n",
    "- we don't have \"number of obs\" (but maybe we just don't have it for this data... F gases use Medusa)\n",
    "- can use resampling methods from openghg... this might just be an approximation though\n",
    "\n",
    "What is needed for post-processing?\n",
    "- fluxes\n",
    "- obs and obs uncertainties\n",
    "- release lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import xarray as xr\n",
    "\n",
    "# function for pruning data vars...\n",
    "def filter_data_vars(ds: xr.Dataset, cond: Callable[[str], bool]) -> xr.Dataset:\n",
    "    keep_dvs = [dv for dv in ds.data_vars if cond(str(dv))]\n",
    "    return ds[keep_dvs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_var(dv: str) -> bool:\n",
    "    return dv not in (\"fp\", \"mf_mod\", \"bc_mod\") and \"particle\" not in dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.map_over_datasets(filter_data_vars, store_data_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test new helper function\n",
    "dt = create_merged_data(params, chunks={\"time\": 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt.map_over_datasets(filter_data_vars, store_data_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Storing data\n",
    "\n",
    "We'll store this data in `/user/work/bm13805/sf6_model_testing_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_path = Path(\"/user/work/bm13805/\")\n",
    "data_path = work_path / \"sf6_model_testing_data\"\n",
    "output_name = \"4h-no-basis-no-filt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.nbytes / 1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    processes=4,\n",
    "    cores=8,\n",
    "    memory='40GB',\n",
    "    walltime='01:00:00',\n",
    "    account=\"chem007981\",\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs = len(ini_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = partial(create_and_save_merged_data, merged_data_dir=data_path, output_name=output_name, chunks={\"time\": 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=njobs)\n",
    "#cluster.scale(memory=\"40GB\")\n",
    "#cluster.adapt(minimum=0, maximum=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip on done, in the background\n",
    "import concurrent.futures\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Any\n",
    "\n",
    "from dask.distributed import Future as DaskFuture\n",
    "\n",
    "\n",
    "def zip_dir_no_compress(store_dir: str | Path, zip_path: str | Path, remove_source: bool = False) -> None:\n",
    "    store_dir = Path(store_dir)\n",
    "    zip_path = Path(zip_path)\n",
    "\n",
    "    if not store_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Store directory not found: {store_dir}\")\n",
    "\n",
    "    # create temp file in same directory as zip_path (use str(...) for tempfile)\n",
    "    fd, tmp_name = tempfile.mkstemp(suffix='.zip', dir=str(zip_path.parent))\n",
    "    os.close(fd)\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(tmp_name, mode='w', compression=zipfile.ZIP_STORED, allowZip64=True) as zf:\n",
    "            for p in sorted(store_dir.rglob('*')):\n",
    "                if p.is_file():\n",
    "                    zf.write(p, arcname=p.relative_to(store_dir))\n",
    "\n",
    "        # quick integrity check\n",
    "        with zipfile.ZipFile(tmp_name, 'r') as zf:\n",
    "            bad = zf.testzip()\n",
    "            if bad is not None:\n",
    "                raise RuntimeError(f\"Corrupt file in zip: {bad}\")\n",
    "\n",
    "        # atomic replace (Path is fine here on modern Python)\n",
    "        os.replace(tmp_name, str(zip_path))\n",
    "\n",
    "        if remove_source:\n",
    "            shutil.rmtree(store_dir)\n",
    "    except Exception:\n",
    "        if os.path.exists(tmp_name):\n",
    "            try:\n",
    "                os.remove(tmp_name)\n",
    "            except Exception:\n",
    "                pass\n",
    "        raise\n",
    "        \n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "zip_dir = partial(zip_dir_no_compress, remove_source=True)\n",
    "\n",
    "def on_done(future: DaskFuture) -> None:\n",
    "    try:\n",
    "        store_dir = future.result()\n",
    "    except Exception as exc:\n",
    "        print(\"Task failed:\", exc)\n",
    "    else:\n",
    "        store_dir = Path(store_dir)\n",
    "        zip_path = store_dir.with_suffix(store_dir.suffix + \".zip\")\n",
    "        \n",
    "        # Submit heavy I/O (zipping) to a background thread pool\n",
    "        executor.submit(zip_dir, store_dir, zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(func, ini_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in futures:\n",
    "    f.add_done_callback(on_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ini_file, future in zip(ini_files, futures):\n",
    "#    future.cancel()\n",
    "    print(ini_file.split(\"/\")[-1], future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsh {data_path} | grep -E \"sf6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_delete = !ls {data_path} | grep -E \"sf6.*zip\"\n",
    "#for f in to_delete:\n",
    "#    !rm {data_path / f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = !ls {data_path} | grep -E \"sf6.*zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(data_path / results[0]) as zf:\n",
    "    for x in zf.infolist():\n",
    "        print(x)\n",
    "#dt_2015 = xr.open_datatree(data_path / \"sf6_2015-01-01_4h-no-basis-no-filt_merged-data.zarr.zip\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test for saving and loading to/from zarr ZipStore\n",
    "\n",
    "...I needed to pass `engine=\"zarr\"` to `xr.open_datatree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {data_path/\"test.zarr.zip\"}\n",
    "def test_zip_store(ini_file):\n",
    "    params0 = read_ini(ini_file)\n",
    "    dt = create_merged_data(params0)\n",
    "    with zarr.ZipStore(data_path / \"test.zarr.zip\", mode=\"w\") as store:\n",
    "        dt.drop_nodes(\"scenario\").to_zarr(store, mode=\"w\")\n",
    "future = client.submit(test_zip_store, ini_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dt = xr.open_datatree(data_path / \"test.zarr.zip\", engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = !ls {data_path} | grep -E \"sf6.*zip\"\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = xr.open_datatree(data_path / results[0], engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.scenario.MHD.fp_x_flux.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.scenario.TAC.fp_x_flux.chunksizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Loading merged data\n",
    "\n",
    "We need to specify engine=\"zarr\" and chunks={}, plus look up the name we created, so it will be useful to have a helper function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "merged_data_name_pat = re.compile(r\"(?P<species>[a-zA-Z0-9]+)_(?P<start_date>[\\d-]+)_(?P<output_name>.+)_merged-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0])\n",
    "print(merged_data_name_pat)\n",
    "print(merged_data_name_pat.search(results[0]).groupdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_name_pat.search(\"ch4_2020-01-01_output_name_merged-data\").groupdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_info = search_merged_data(data_path)\n",
    "md_info[\"ext\"] = md_info[\"path\"].apply(lambda x: x.suffix)\n",
    "md_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhst {data_path/\"sf6_2016-01-01_4h-no-basis-no-filt_merged-data.zarr\"/\"scenario\"/\"CMN\"/\"fp_x_flux\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "So some of the zipped data has been duplicated... this seems bad.\n",
    "\n",
    "Does this happen when I open the data? 2017 doesn't have an unzipped copy, so I can try opening that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_info.loc[4, \"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2017 = xr.open_datatree(data_path / md_info.loc[4, \"path\"], engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lst {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "What if I open without chunks={}?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2017 = xr.open_datatree(data_path / md_info.loc[4, \"path\"], engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2017.scenario.TAC.fp_x_flux.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsht {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "From an earlier cell, I can see that right after the futures finished, eventually I had just the .zarr.zip results. Maybe the processes that were removing the .zarr files failed somehow?\n",
    "\n",
    "Or maybe some older jobs completed?\n",
    "\n",
    "Is the zipped data okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2015_1 = xr.open_datatree(data_path / md_info.iloc[0, -2], engine=\"zarr\")\n",
    "dt_2015_2 = xr.open_datatree(data_path / md_info.iloc[1, -2], engine=\"zarr\")\n",
    "xr.testing.assert_isomorphic(dt_2015_1, dt_2015_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Okay so we can delete the unzipped data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_raw = !ls -lsht {data_path}\n",
    "files = [fr.strip().split()[-1] for fr in files_raw]\n",
    "files\n",
    "\n",
    "#for f in files[1:4]:\n",
    "#    !rm -rf {data_path/f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2021 = load_merged_data(data_path, start_date=\"2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
