{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from openghg.util import split_function_inputs\n",
    "import xarray as xr\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import zipfile\n",
    "import zarr\n",
    "import re\n",
    "\n",
    "from inversions.dask_helpers import zip_on_done\n",
    "\n",
    "# replacement for %run\n",
    "from inversions.data_functions import (\n",
    "    read_ini,\n",
    "    data_processing,\n",
    "    fp_all_to_datatree,\n",
    "    filter_data_vars,\n",
    "    store_data_var,\n",
    "    create_merged_data,\n",
    "    create_and_save_merged_data,\n",
    "    search_merged_data,\n",
    "    load_merged_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Data for SF6 tests\n",
    "\n",
    "I already saved some data using 4h averaging and 250 basis functions, along with the filters we typically use for PARIS.\n",
    "\n",
    "It would be helpful to save the data in a state where I could change the number of basis functions or filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sf6_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/\")\n",
    "sf6_base_nid2025_path = sf6_path / \"RHIME_NAME_EUROPE_FLAT_ConfigNID2025_sf6_yearly\"\n",
    "ini_files = [p.name for p in sf6_base_nid2025_path.glob(\"*.ini\")]\n",
    "# get 2015-2024\n",
    "ini_files = ini_files[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ini_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Test for MultiObs and MultiFootprint with ModelScenario\n",
    "\n",
    "Alignment failed when trying to pass MultiObs and MultiFootprint directly to ModelScenario, so I resorted to using a loop over sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = read_ini(ini_files[0])\n",
    "data_params, _ = split_function_inputs(params, data_processing)\n",
    "\n",
    "fp_all = data_processing(**data_params)\n",
    "\n",
    "dt = fp_all_to_datatree(fp_all, rechunk=False)\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 2
   },
   "source": [
    "## Storing this data for later processing\n",
    "\n",
    "If I want to store the data without applying basis functions or filtering, what do I need to store?\n",
    "- need mean fp x mean flux, but we can get this from fp x flux...\n",
    "- need some footprint info for filters (mostly met. data, only need `fp` for `local_influence` filter)\n",
    "\n",
    "What if we want to resample again?\n",
    "- we don't have \"number of obs\" (but maybe we just don't have it for this data... F gases use Medusa)\n",
    "- can use resampling methods from openghg... this might just be an approximation though\n",
    "\n",
    "What is needed for post-processing?\n",
    "- fluxes\n",
    "- obs and obs uncertainties\n",
    "- release lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test new helper function\n",
    "dt = create_merged_data(params, chunks={\"time\": 400})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt.map_over_datasets(filter_data_vars, store_data_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Storing data\n",
    "\n",
    "We'll store this data in `/user/work/bm13805/sf6_model_testing_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "work_path = Path(\"/user/work/bm13805/\")\n",
    "data_path = work_path / \"sf6_model_testing_data\"\n",
    "output_name = \"4h-no-basis-no-filt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster = SLURMCluster(\n",
    "    processes=4,\n",
    "    cores=8,\n",
    "    memory=\"40GB\",\n",
    "    walltime=\"01:00:00\",\n",
    "    account=\"chem007981\",\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs = len(ini_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = partial(\n",
    "    create_and_save_merged_data,\n",
    "    merged_data_dir=data_path,\n",
    "    output_name=output_name,\n",
    "    chunks={\"time\": 400},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "cluster.scale(jobs=njobs)\n",
    "# cluster.scale(memory=\"40GB\")\n",
    "# cluster.adapt(minimum=0, maximum=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(func, ini_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in futures:\n",
    "    f.add_done_callback(zip_on_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ini_file, future in zip(ini_files, futures):\n",
    "    #    future.cancel()\n",
    "    print(ini_file.split(\"/\")[-1], future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsh {data_path} | grep -E \"sf6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_delete = !ls {data_path} | grep -E \"sf6.*zip\"\n",
    "# for f in to_delete:\n",
    "#    !rm {data_path / f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [p.name for p in data_path.glob(\"sf6*zip\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with zipfile.ZipFile(data_path / results[0]) as zf:\n",
    "    for x in zf.infolist():\n",
    "        print(x)\n",
    "# dt_2015 = xr.open_datatree(data_path / \"sf6_2015-01-01_4h-no-basis-no-filt_merged-data.zarr.zip\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "lines_to_next_cell": 2
   },
   "source": [
    "## Test for saving and loading to/from zarr ZipStore\n",
    "\n",
    "...I needed to pass `engine=\"zarr\"` to `xr.open_datatree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {data_path/\"test.zarr.zip\"}\n",
    "def test_zip_store(ini_file):\n",
    "    params0 = read_ini(ini_file)\n",
    "    dt = create_merged_data(params0)\n",
    "    with zarr.ZipStore(data_path / \"test.zarr.zip\", mode=\"w\") as store:\n",
    "        dt.drop_nodes(\"scenario\").to_zarr(store, mode=\"w\")\n",
    "\n",
    "\n",
    "future = client.submit(test_zip_store, ini_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dt = xr.open_datatree(data_path / \"test.zarr.zip\", engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Checking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = !ls {data_path} | grep -E \"sf6.*zip\"\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = xr.open_datatree(data_path / results[0], engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.scenario.MHD.fp_x_flux.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.scenario.TAC.fp_x_flux.chunksizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Loading merged data\n",
    "\n",
    "We need to specify engine=\"zarr\" and chunks={}, plus look up the name we created, so it will be useful to have a helper function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_data_name_pat = re.compile(\n",
    "    r\"(?P<species>[a-zA-Z0-9]+)_(?P<start_date>[\\d-]+)_(?P<output_name>.+)_merged-data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0])\n",
    "print(merged_data_name_pat)\n",
    "print(merged_data_name_pat.search(results[0]).groupdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_name_pat.search(\"ch4_2020-01-01_output_name_merged-data\").groupdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_info = search_merged_data(data_path)\n",
    "md_info[\"ext\"] = md_info[\"path\"].apply(lambda x: x.suffix)\n",
    "md_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhst {data_path/\"sf6_2016-01-01_4h-no-basis-no-filt_merged-data.zarr\"/\"scenario\"/\"CMN\"/\"fp_x_flux\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "So some of the zipped data has been duplicated... this seems bad.\n",
    "\n",
    "Does this happen when I open the data? 2017 doesn't have an unzipped copy, so I can try opening that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_info.loc[4, \"path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2017 = xr.open_datatree(data_path / md_info.loc[4, \"path\"], engine=\"zarr\", chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lst {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "What if I open without chunks={}?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2017 = xr.open_datatree(data_path / md_info.loc[4, \"path\"], engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2017.scenario.TAC.fp_x_flux.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsht {data_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "From an earlier cell, I can see that right after the futures finished, eventually I had just the .zarr.zip results. Maybe the processes that were removing the .zarr files failed somehow?\n",
    "\n",
    "Or maybe some older jobs completed?\n",
    "\n",
    "Is the zipped data okay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2015_1 = xr.open_datatree(data_path / md_info.iloc[0, -2], engine=\"zarr\")\n",
    "dt_2015_2 = xr.open_datatree(data_path / md_info.iloc[1, -2], engine=\"zarr\")\n",
    "xr.testing.assert_isomorphic(dt_2015_1, dt_2015_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Okay so we can delete the unzipped data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_raw = [p.name for p in data_path.iterdir()]\n",
    "files = [fr.strip().split()[-1] for fr in files_raw]\n",
    "files\n",
    "\n",
    "# for f in files[1:4]:\n",
    "#    !rm -rf {data_path/f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2021 = load_merged_data(data_path, start_date=\"2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
