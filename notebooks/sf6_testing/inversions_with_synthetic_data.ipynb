{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tests on synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Existing models, EDGAR synthetic data\n",
    "\n",
    "### Plan\n",
    "\n",
    "- create merged data with all variations of obs (with a \"instrument\" or \"experiment\" dimension)\n",
    "  - this should do basis functions before filtering, like current setup\n",
    "  - use same filtering options\n",
    "  - start with merged data with fp_x_flux\n",
    "  - resample synth obs to 4h\n",
    "- make code to load data, create inputs, build model, and save outputs\n",
    "- make model config settings\n",
    "- configure output locations for each config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Making merged data with all synth obs\n",
    "\n",
    "The merged data w/o basis and filtering is stored by year, it is probably easiest to match this and use a different \"output_name\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inversions_experimental_code/data_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "#### Getting all synth obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.retrieve import search_surface, get_obs_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_surface(species=\"sf6\", store=\"sf6_testing_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.results.groupby(\"site\").instrument.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(res.results.instrument.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "We'll parse these for noise and bias values as floats, then add these as dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = sorted(res.results.site.unique())\n",
    "instruments = sorted(res.results.instrument.unique())\n",
    "\n",
    "instr_to_dims = {}\n",
    "for instr in instruments:\n",
    "    noise, bias = instr.split(\"_\")[-2:]\n",
    "    noise_val = 0.0 if noise.startswith(\"no\") else float(noise.removesuffix(\"-noise\")) / 10\n",
    "    bias_val = 0.0 if bias.startswith(\"no\") else float(bias.removesuffix(\"-bias\")) / 100\n",
    "    instr_to_dims[instr] = {\"noise\": [noise_val], \"bias\": [bias_val]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = []\n",
    "other_args = {\"start_date\": \"2015-01-01\",\n",
    "              \"end_date\": \"2025-01-01\", \n",
    "              \"inlets\": [None] * len(sites), \n",
    "              \"obs_data_levels\": [None] * len(sites),\n",
    "              \"averaging_periods\": [\"4h\"] * len(sites),\n",
    "             }\n",
    "for instr in instruments:\n",
    "    multi_obs = MultiObs(species=\"sf6\", sites=sites, instruments=[instr] * len(sites), **other_args)\n",
    "    all_obs.append(multi_obs.data.expand_dims(instr_to_dims[instr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "combined_obs = xr.combine_by_coords(all_obs, combine_attrs=\"drop_conflicts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Loading merged data\n",
    "\n",
    "Once data is in, we need to compute basis functions.\n",
    "\n",
    "We maybe also need this data for computing outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"/user/work/bm13805/sf6_model_testing_data/\")\n",
    "md_res = search_merged_data(data_path)\n",
    "md_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged_data = [load_merged_data(merged_data_dir=data_path, species=\"sf6\", start_date=start_date) for start_date in md_res.start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Making and applying basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg_inversions.basis.algorithms import weighted_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "intem_regions = xr.open_dataset(\"/user/work/bm13805/openghg_inversions/openghg_inversions/basis/outer_region_definition_EUROPE.nc\").region\n",
    "_, intem_regions = xr.align(all_merged_data[0].flux.to_dataset(), intem_regions, join=\"override\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_fp_x_flux(dt: xr.DataTree, mask: xr.DataArray | None = None) -> xr.DataArray:\n",
    "    if mask is not None:\n",
    "        ds_list = [v.fp_x_flux.where(mask, drop=True).expand_dims({\"site\": [k]}) for k, v in dt.scenario.items()]\n",
    "    else:\n",
    "        ds_list = [v.fp_x_flux.expand_dims({\"site\": [k]}) for k, v in dt.scenario.items()]\n",
    "    return xr.concat(ds_list, dim=\"site\").mean([\"site\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(mean_fp_x_flux(all_merged_data[0]).compute()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(all_merged_data[0].flux[\"flat-annual-total\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def weighted_fixed_outer_regions_basis(merged_data: xr.DataTree, nbasis: int = 250, domain: str = \"EUROPE\") -> xr.DataArray:\n",
    "    intem_regions = xr.open_dataset(\"/user/work/bm13805/openghg_inversions/openghg_inversions/basis/outer_region_definition_EUROPE.nc\").region\n",
    "    _, intem_regions = xr.align(merged_data.flux.to_dataset(), intem_regions, join=\"override\")\n",
    "\n",
    "    inner_index = intem_regions.max().values\n",
    "    mask = intem_regions == inner_index\n",
    "    grid = mean_fp_x_flux(merged_data, mask=mask)\n",
    "    grid = grid / grid.max()\n",
    "\n",
    "    func = partial(weighted_algorithm, nregion=nbasis, bucket=1, domain=domain)\n",
    "    inner_region = xr.apply_ufunc(func, grid.as_numpy()).rename(\"basis\")\n",
    "\n",
    "    basis = intem_regions.rename(\"basis\") \n",
    "\n",
    "    loc_dict = {\n",
    "        \"lat\": slice(inner_region.lat.min(), inner_region.lat.max() + 0.1),\n",
    "        \"lon\": slice(inner_region.lon.min(), inner_region.lon.max() + 0.1),\n",
    "    }\n",
    "    basis.loc[loc_dict] = (inner_region + inner_index-1).squeeze().values\n",
    "\n",
    "    basis += 1  # intem_region_definitions.nc regions start at 0, not 1\n",
    "\n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_functions = [weighted_fixed_outer_regions_basis(merged_data) for merged_data in all_merged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run inversions_experimental_code/basis_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf1 = BasisFunctions(basis_functions[0], all_merged_data[0].flux[\"flat-annual-total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "world = gpd.read_file(\"natural_earth_50.zip\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "lat_min, lat_max = 40, 70\n",
    "lon_min, lon_max = -20, 20\n",
    "\n",
    "#lat_min, lat_max = basis_functions[0].lat.min().values, basis_functions[0].lat.max().values\n",
    "#lon_min, lon_max = basis_functions[0].lon.min().values, basis_functions[0].lon.max().values\n",
    "\n",
    "\n",
    "bf1.plot(shuffle=True)\n",
    "world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "ax.set_xlim(float(lon_min), float(lon_max))\n",
    "ax.set_ylim(float(lat_min), float(lat_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged_data[0].scenario.map_over_datasets(lambda ds: bf1.sensitivities(ds.fp_x_flux).rename(\"H\").to_dataset() if \"fp_x_flux\" in ds.data_vars else ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_objs = [BasisFunctions(bf, md.flux[\"flat-annual-total\"]) for bf, md in zip(basis_functions, all_merged_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_basis_functions(ds: xr.Dataset, bf: BasisFunctions) -> xr.Dataset:\n",
    "    if \"fp_x_flux\" not in ds:\n",
    "        return ds\n",
    "    return bf.sensitivities(ds.fp_x_flux).rename(\"H\").to_dataset()\n",
    "\n",
    "h_matrix_datatrees = [md.map_over_datasets(partial(apply_basis_functions, bf=bf)) for md, bf in zip(all_merged_data, bf_objs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_matrix_datatrees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dict(ds_dict: dict[str, xr.Dataset], dim: str) -> xr.Dataset:\n",
    "    return xr.concat([v.expand_dims({dim: [k]}) for k, v in ds_dict.items()], dim=dim)\n",
    "\n",
    "def concat_tree(dt: xr.DataTree, dim: str) -> xr.Dataset:\n",
    "    ds_dict = {k: v.to_dataset() for k, v in dt.items()}\n",
    "    return concat_dict(ds_dict, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_matrices = [concat_tree(dt.scenario, \"site\") for dt in h_matrix_datatrees]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_matrices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesw_bc_basis(ds: xr.Dataset) -> xr.DataArray:\n",
    "    bc_ds = ds[[f\"bc_{d}\" for d in \"nesw\"]].rename({f\"bc_{d}\": d for d in \"nesw\"})\n",
    "    return bc_ds.sum([\"lat\", \"lon\", \"height\"]).to_dataarray(dim=\"bc_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_bc_matrices = [concat_tree(md.scenario.map_over_datasets(lambda ds: nesw_bc_basis(ds).rename(\"H_bc\").to_dataset() if \"bc_n\" in ds else ds), dim=\"site\")\n",
    "                 for md in all_merged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_bc_matrices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Adding mf_error, splitting obs by year, and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_obs[\"mf_repeatability\"] = combined_obs[\"mf_repeatability\"].astype(\"float32\")\n",
    "combined_obs[\"mf_error\"] = np.sqrt(combined_obs.mf_repeatability**2 + combined_obs.mf_variability**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def make_dates_df(\n",
    "    year: int,\n",
    "    n_periods: int,\n",
    "    frequency: Literal[\"annual\", \"yearly\", \"monthly\"] = \"annual\",\n",
    "    initial_month: int = 1,\n",
    "    array_job_id: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a DataFrame containing `n_periods` start and end dates starting at the given\n",
    "    `year` and initial month (`initial_month` defaults to 1).\n",
    "\n",
    "    Args:\n",
    "        year: year to start first period\n",
    "        n_periods: number of periods (months or years) in result\n",
    "        frequency: length of periods, \"annual\" or \"monthly\"\n",
    "        initial_month: month to start first period (default 1)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing columns for start and end dates.\n",
    "    \"\"\"\n",
    "    if frequency in [\"annual\", \"yearly\"]:\n",
    "        freq = \"YS\"\n",
    "        n_years, n_months = n_periods, 0\n",
    "        offset = pd.DateOffset(years=1)  # offset for start vs. end dates\n",
    "    elif frequency == \"monthly\":\n",
    "        freq = \"MS\"\n",
    "        n_years, n_months = 0, n_periods\n",
    "        offset = pd.DateOffset(months=1)  # offset for start vs. end dates\n",
    "    else:\n",
    "        raise ValueError(f\"Frequency {frequency} not accepted.\")\n",
    "\n",
    "    start = pd.Timestamp(year, initial_month, 1)\n",
    "    end = start + pd.DateOffset(years=n_years, months=n_months)  # type: ignore\n",
    "\n",
    "    start_dates = pd.date_range(start, end, inclusive=\"left\", freq=freq)\n",
    "    end_dates = start_dates + offset\n",
    "\n",
    "    dates_df = pd.DataFrame({\"start_date\": start_dates, \"end_date\": end_dates})\n",
    "\n",
    "    if array_job_id:\n",
    "        dates_df.index +=1\n",
    "        dates_df = dates_df.rename_axis(\"array_job_id\")\n",
    "\n",
    "    return dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = []\n",
    "for _, s, e in make_dates_df(2015, 10, array_job_id=False).itertuples():\n",
    "    slices.append(slice(s, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_obs = [combined_obs.sel(time=s) for s in slices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_obs = [so.assign_coords(site=split_obs[0].site.str.upper()) for so in split_obs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# add info for later filtering\n",
    "inlet_infos = []\n",
    "for md in all_merged_data:\n",
    "    inlet_info = defaultdict(list)\n",
    "    columns = [\"inlet_height_magl\", \"inlet_latitude\", \"inlet_longitude\"]\n",
    "    for k, v in md.scenario.items():\n",
    "        for col in columns:\n",
    "            try:\n",
    "                val = float(v.attrs.get(col, np.nan))\n",
    "            except:\n",
    "                val = np.nan\n",
    "            inlet_info[k].append(val)\n",
    "    df = pd.DataFrame.from_dict(inlet_info).T\n",
    "    df.columns = columns\n",
    "    df.index.name = \"site\"\n",
    "    inlet_infos.append(df.to_xarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(ds):\n",
    "    dvs = [dv for dv in ds.data_vars if (ds[dv].dims == (\"time\",)) & (not str(dv).startswith(\"mf\"))]\n",
    "    return ds[dvs]\n",
    "\n",
    "met_data = [concat_tree(md.scenario.map_over_datasets(func), dim=\"site\") for md in all_merged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [xr.merge([h, hbc, obs, iinfo, met], join=\"left\") for h, hbc, obs, iinfo, met in zip(h_matrices, h_bc_matrices, split_obs, inlet_infos, met_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0].where((all_data[0].atmosphere_boundary_layer_thickness.compute() > 200.0) \n",
    "                  | (all_data[0].site.isin((\"CMN\", \"JFJ\"))\n",
    "                  | (all_data[0].atmosphere_boundary_layer_thickness.compute() > 50.0 + all_data[0].inlet_height_magl.compute())  \n",
    "                    )).stack(nmeasure=(\"site\", \"time\")).dropna(\"nmeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_time(data):\n",
    "    return data.time + xr.apply_ufunc(lambda x: pd.to_timedelta(24 * 60 * x / 360.0, unit=\"h\"), data.inlet_longitude)\n",
    "\n",
    "def local_hour(data):\n",
    "    return local_time(data).dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_time(all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg_inversions.hbmcmc.hbmcmc_output import ncdf_encoding\n",
    "\n",
    "\n",
    "for data in all_data:\n",
    "    start_date = str(data.time.values[0]).split(\"T\")[0]\n",
    "    output_name = f\"sf6_{start_date}_synth_merged.nc\"\n",
    "    encoding = ncdf_encoding(data)\n",
    "    data.to_netcdf(data_path / output_name, encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsth {data_path}\n",
    "#files = !ls {data_path} | grep synth_merged\n",
    "#for file in files:\n",
    "#    !rm {data_path / file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(data_path / \"sf6_2015-01-01_synth_merged.nc\") as ds:\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### I forgot to filter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "We need PBLH aligned with this style of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(ds):\n",
    "    dvs = [dv for dv in ds.data_vars if (ds[dv].dims == (\"time\",)) & (not str(dv).startswith(\"mf\"))]\n",
    "    return ds[dvs]\n",
    "\n",
    "met_data = [concat_tree(md.scenario.map_over_datasets(func), dim=\"site\") for md in all_merged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "met_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0].where((met_data[0].atmosphere_boundary_layer_thickness.compute() > 200.0) | all_data[0].site.isin([\"CMN\", \"JFJ\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlet_height_magl = {}\n",
    "for md in all_merged_data:\n",
    "    for k, v in md.scenario.items():\n",
    "        if \"inlet_height_magl\" in v.attrs:\n",
    "            inlet_height_magl[k] = v.attrs[\"inlet_height_magl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ihms = pd.Series(inlet_height_magl)\n",
    "ihms.index.name = \"site\"\n",
    "inlet_height_da = ihms.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0].where((met_data[0].atmosphere_boundary_layer_thickness.compute() > 50.0 + inlet_height_da) | all_data[0].site.isin([\"CMN\", \"JFJ\"]), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pblh_min_filter(data, met, threshold=200.0, no_filter: list[str] | None = None):\n",
    "    no_filter = no_filter or []\n",
    "    return data.where((met.atmosphere_boundary_layer_thickness.compute() > threshold) | data.site.isin(no_filter))\n",
    "\n",
    "def pblh_diff_filter(data, met, diff_threshold=200.0, no_filter: list[str] | None = None):\n",
    "    no_filter = no_filter or []\n",
    "    return data.where((met.atmosphere_boundary_layer_thickness.compute() > diff_threshold + inlet_height_da) | data.site.isin(no_filter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = []\n",
    "\n",
    "for data, met in zip(all_data, met_data):\n",
    "    fdata = pblh_min_filter(data, met, no_filter=[\"JFJ\", \"CMN\"])\n",
    "    fdata = pblh_diff_filter(fdata, met, no_filter=[\"JFJ\", \"CMN\"])\n",
    "    filtered_data.append(fdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data[0].stack(nmeasure=(\"site\", \"time\")).dropna(\"nmeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### Running inversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "#### Making inversion inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run likelihood_tests.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "sf6_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/\")\n",
    "sf6_base_nid2025_path = sf6_path / \"RHIME_NAME_EUROPE_FLAT_ConfigNID2025_sf6_yearly\"\n",
    "ini_files = !ls {sf6_base_nid2025_path / \"*.ini\"}\n",
    "# get 2015-2024\n",
    "ini_files = ini_files[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pblh_filters(ds: xr.Dataset, no_filter=(\"CMN\", \"JFJ\"), pblh_min_thres: float = 200.0, pblh_diff_thres: float = 50.0) -> xr.Dataset:\n",
    "    pblh_min_filt = ds.atmosphere_boundary_layer_thickness.compute() > pblh_min_thres\n",
    "    pblh_diff_filt = ds.atmosphere_boundary_layer_thickness.compute() > pblh_diff_thres + ds.inlet_height_magl.compute()\n",
    "    no_filt = ds.site.isin(no_filter)\n",
    "    return ds.where(no_filt | (pblh_min_filt & pblh_diff_filt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_error_method(ds: xr.Dataset) -> np.ndarray:\n",
    "    mf = ds.mf.as_numpy().sortby(\"time\")\n",
    "    monthly_50pc = mf.resample(time=\"MS\").quantile(0.5)\n",
    "    monthly_5pc = mf.resample(time=\"MS\").quantile(0.05)\n",
    "    res_err = (monthly_50pc - monthly_5pc).groupby(\"site\").mean(dim=\"time\")\n",
    "\n",
    "    return res_err.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_inversion_data_vars(ds: xr.Dataset) -> xr.Dataset:\n",
    "    inversion_data_vars = [\"H\", \"H_bc\", \"mf\", \"mf_error\", \"mf_repeatability\", \"mf_variability\"]\n",
    "    dvs = [dv for dv in ds.data_vars if dv in inversion_data_vars]\n",
    "    return ds[dvs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_inv_inputs2(\n",
    "    ds: xr.Dataset,\n",
    "    bc_freq: str | None = None,\n",
    "    sigma_freq: str | None = None,\n",
    "    min_error: str | dict[str, float] | float = 0.0,\n",
    "    min_error_per_site: bool = True,\n",
    "):\n",
    "    # compute min error values (do this before stacking)\n",
    "    if isinstance(min_error, float):\n",
    "        ds[\"min_error\"] = min_error * xr.ones_like(ds.mf)\n",
    "    elif isinstance(min_error, dict):\n",
    "        sites = np.unique(ds.site)\n",
    "        min_err_values = np.array([min_error[site] for site in sites])\n",
    "    elif min_error == \"percentile\":\n",
    "        min_err_values = percentile_error_method(ds)\n",
    "    else:\n",
    "        raise ValueError(f\"Option '{min_error}' is not valid.\")\n",
    "\n",
    "    # stack\n",
    "    ds = ds.stack(nmeasure=(\"site\", \"time\")).dropna(\"nmeasure\")\n",
    "\n",
    "    # add BC freq\n",
    "    if bc_freq is not None:\n",
    "        temp = setup_bc(ds.H_bc, bc_freq)\n",
    "        ds = ds.drop_dims(\"bc_region\")\n",
    "        ds[\"H_bc\"] = temp\n",
    "\n",
    "    ds[\"site_indicator\"] = make_site_indicator(ds.site)\n",
    "    ds[\"sigma_freq_index\"] = setup_sigma_freq(ds.time, freq=sigma_freq)\n",
    "\n",
    "    # set up min error in more complicated cases\n",
    "    if \"min_error\" not in ds:\n",
    "        def setup_min_error2(min_err_values, site_indicator):\n",
    "            return min_err_values[..., site_indicator]\n",
    "        ds[\"min_error\"] = xr.apply_ufunc(lambda x: setup_min_error2(min_err_values, x), ds.site_indicator, input_core_dims=[[\"nmeasure\"]], output_core_dims=[list(ds.mf.dims)])\n",
    "\n",
    "\n",
    "    #ds[\"basis_flat\"] = fp_data[\".basis\"]\n",
    "\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = read_ini(ini_files[0])\n",
    "params.get(\"calculate_min_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_inv_input(data: xr.Dataset, bc_freq, sigma_freq, min_error) -> xr.Dataset:\n",
    "    return (data\n",
    "        .pipe(pblh_filters)\n",
    "        .pipe(select_inversion_data_vars)\n",
    "        .pipe(make_inv_inputs2, bc_freq=bc_freq, sigma_freq=sigma_freq, min_error=min_error)\n",
    "        ).compute().dropna(\"nmeasure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Making models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhime_model(inv_input: xr.Dataset, x_prior: dict, bc_prior: dict, sig_prior: dict, offset: bool = True, pefo: bool = True) -> pm.Model:\n",
    "    with pm.Model() as model:\n",
    "        mu = add_linear_component(\n",
    "            inv_input.H,\n",
    "            data_name=\"hx\",\n",
    "            prior_args=x_prior,\n",
    "            var_name=\"x\",\n",
    "            output_name=\"mu\",\n",
    "        )\n",
    "        mu_bc = add_linear_component(\n",
    "            inv_input.H_bc,\n",
    "            data_name=\"hbc\",\n",
    "            prior_args=bc_prior,\n",
    "            var_name=\"bc\",\n",
    "            output_name=\"mu_bc\",\n",
    "            compute_deterministic=True,\n",
    "        )\n",
    "\n",
    "        if offset:\n",
    "            mu_bc = mu_bc + make_offset(inv_input.site_indicator, {\"pdf\": \"normal\"})\n",
    "\n",
    "        add_old_likelihood(inv_input, sig_prior, mu=mu, mu_bc=mu_bc, power=1.99, pollution_events_from_obs=pefo)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "#### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_configs = pd.DataFrame([{k: v[0] for k, v in val.items()} for val in instr_to_dims.values()])\n",
    "experiment_configs[\"pefo\"] = [[True, False]] * 30\n",
    "experiment_configs[\"offset\"] = [[True, False]] * 30\n",
    "experiment_configs = experiment_configs.explode(\"pefo\", ignore_index=True)\n",
    "experiment_configs = experiment_configs.explode(\"offset\", ignore_index=True)\n",
    "experiment_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment_configs.to_csv(data_path / \"experiment_configs1.csv\")\n",
    "experiment_configs = pd.read_csv(data_path / \"experiment_configs1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec1_subset = experiment_configs.loc[(experiment_configs.bias.isin([0.0, -0.1])) & (experiment_configs.noise.isin([0.0, 0.6, 2.0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec1_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ec1_subset.iloc[0]\n",
    "for num, row in ec1_subset.iterrows():\n",
    "    print(num, row, type(row))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_data_args(params: dict) -> dict:\n",
    "    result = dict(\n",
    "        bc_freq=params.get(\"bc_freq\"), \n",
    "        sigma_freq=params.get(\"sigma_freq\"), \n",
    "        min_error=params.get(\"calculate_min_error\") or params.get(\"min_error\", 0.0),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_args(params: dict, exp_conf: dict) -> dict:\n",
    "    result = {\n",
    "        \"x_prior\": params.get(\"xprior\"),\n",
    "        \"bc_prior\": params.get(\"bcprior\"),\n",
    "        \"sig_prior\": params.get(\"sigprior\"),\n",
    "        \"offset\": exp_conf.get(\"offset\", True),\n",
    "        \"pefo\": exp_conf.get(\"pefo\", True),\n",
    "    }\n",
    "    if result[\"x_prior\"].get(\"pdf\", \"\").lower() == \"lognormal\" and params.get(\"reparameterise_log_normal\"):\n",
    "        result[\"x_prior\"][\"reparameterise\"] = True\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_conf = dict(ec1_subset.iloc[0])\n",
    "params = read_ini(ini_files[0])\n",
    "\n",
    "inv_input = filtered_inv_input(all_data[0], **base_data_args(params)).sel(noise=exp_conf[\"noise\"], bias=exp_conf[\"bias\"])\n",
    "model = rhime_model(inv_input, **model_args(params, exp_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "#### Set-up sampling params and output options\n",
    "\n",
    "The plan is to dump the raw traces, tagged by year and \"experiment config number\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"/group/chem/acrg/PARIS_inversions/sf6/brendan_tests\")\n",
    "!ls {out_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = out_path / \"logs\"\n",
    "log_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_kwargs = default_sample_kwargs.copy()\n",
    "sample_kwargs[\"blas_cores\"] = 8\n",
    "sample_kwargs[\"draws\"] = 1000\n",
    "sample_kwargs[\"tune\"] = 1000\n",
    "sample_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_out_name(year, exp_conf_number):\n",
    "    return f\"{year}_config_{exp_conf_number}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "\n",
    "def zarr_trace(out_path: Path, out_name: str):\n",
    "    store = zarr.DirectoryStore(out_path / (out_name + \"_trace.zarr\"))\n",
    "    return pm.backends.zarr.ZarrTrace(store, compressor=pm.util.UNSET, draws_per_chunk=200, include_transformed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inversion(data_path: Path, out_path: Path, exp_num: int, exp_conf: dict, ini_file: str, sample_kwargs: dict, zarr_backend: bool = True, save_trace: bool = True, error_noise: float | None = None):\n",
    "    params = dict(read_ini(ini_file))\n",
    "    year = params[\"start_date\"][:4]\n",
    "    print(f\"Experiment {exp_num}: year {year}, {exp_conf}\")\n",
    "    out_name = make_out_name(year, exp_num)\n",
    "\n",
    "    if save_trace and zarr_backend:\n",
    "        sample_kwargs[\"trace\"] = zarr_trace(out_path, out_name)\n",
    "\n",
    "    merged_data_path = data_path / f\"sf6_{params['start_date']}_synth_merged.nc\"\n",
    "    with xr.open_dataset(merged_data_path, cache=False) as data:\n",
    "        print(f\"Experiment {exp_num} loading data {merged_data_path}\")\n",
    "        inv_input_all = filtered_inv_input(data, **base_data_args(params))\n",
    "        inv_input = inv_input_all.sel(noise=exp_conf[\"noise\"], bias=exp_conf[\"bias\"])\n",
    "\n",
    "        if error_noise is not None:\n",
    "            inv_input[\"mf_error\"] = inv_input_all.mf_error.sel(noise=error_noise, bias=0.0)\n",
    "\n",
    "    inv_input = inv_input.dropna(\"nmeasure\")\n",
    "    print(f\"Experiment {exp_num}, year {Path(merged_data_path).name[:-4]} building model.\")\n",
    "    model = rhime_model(inv_input, **model_args(params, exp_conf))\n",
    "    print(f\"Experiment {exp_num}, year {Path(merged_data_path).name[:-4]} sampling.\")\n",
    "    idata = pm.sample(model=model, **sample_kwargs)\n",
    "    print(f\"Experiment {exp_num}, year {Path(merged_data_path).name[:-4]} done.\")\n",
    "\n",
    "    if save_trace and not zarr_backend:\n",
    "        idata.to_netcdf(out_path / (out_name + \"_trace.nc\"))\n",
    "\n",
    "    if not save_trace:\n",
    "        return idata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "#### Launch jobs\n",
    "\n",
    "- I'm going to run 6 (of 30) data variations for 10 years each, with 4 different models.\n",
    "- I'll use 24 cores, 6 for each chain\n",
    "- Using one process will mean one worker per mcmc run (I hope)\n",
    "- I could use one job per variation and run the years in a loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    processes=1,\n",
    "    cores=8,\n",
    "    memory='20GB',\n",
    "    walltime='02:00:00',\n",
    "    account=\"chem007981\",\n",
    "    log_directory=str(log_path),\n",
    ")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(sorted(cluster.workers.items(), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_workers = [v.get(\"id\") for v in client.scheduler_info(n_workers=len(cluster.workers))[\"workers\"].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(available_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retire bad workers...\n",
    "for k, v in cluster.workers.items():\n",
    "    if k in available_workers:\n",
    "        continue\n",
    "    print(\"Closing\", k)\n",
    "    v.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect workers\n",
    "workers = client.scheduler_info(n_workers=len(cluster.workers))[\"workers\"]  # dict: {worker_address: info}\n",
    "for addr, info in workers.items():\n",
    "    print(addr)\n",
    "    pprint(info)\n",
    "    break\n",
    "    print(\"  host:\", info.get(\"host\"))            # hostname where worker runs\n",
    "    print(\"  pid:\", info.get(\"pid\"))              # worker process id\n",
    "    print(\"  nthreads:\", info.get(\"nthreads\"))\n",
    "    print(\"  memory_limit:\", info.get(\"memory_limit\"))\n",
    "    print(\"  last_seen:\", info.get(\"last-seen\"))  # timestamp (may be milliseconds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_keys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "futures = defaultdict(list)  # futures keyed by experiment config number\n",
    "\n",
    "count = 0\n",
    "workers = available_workers #list(cluster.workers.keys())\n",
    "for ini_file in ini_files[:5]:\n",
    "    for exp_num, row in ec1_subset2.iterrows():\n",
    "        exp_conf = dict(row)\n",
    "\n",
    "        func = partial(run_inversion, \n",
    "                       data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num, \n",
    "                       exp_conf=exp_conf, \n",
    "                       ini_file=ini_file, \n",
    "                       sample_kwargs=sample_kwargs,\n",
    "                       zarr_backend=False,\n",
    "                      )\n",
    "        worker_name = workers[count]\n",
    "        key = f\"exp-{exp_num}_{Path(ini_file).name[:-4]}\"\n",
    "        if key in done_keys or key in processing:\n",
    "            continue\n",
    "        future = client.submit(func, workers=worker_name, key=key)\n",
    "        futures[exp_num].append(future)\n",
    "        time.sleep(2)\n",
    "        count += 1\n",
    "        count = count % len(available_workers)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in futures.values():\n",
    "    for f in v:\n",
    "        if f.key not in done_keys and f.key not in processing:\n",
    "            f.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "#dir(client)\n",
    "pprint(client.processing())\n",
    "pprint(client.futures)\n",
    "#client.retry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close workers we're not using\n",
    "to_close = [k for k, v in client.processing().items() if not v]\n",
    "client.retire_workers(workers=to_close, close_workers=True, remove=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel everything that isn't processing and start again\n",
    "processing = []\n",
    "for v in client.processing().values():\n",
    "    if v:\n",
    "        processing.extend(v)\n",
    "\n",
    "processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in client.futures.items():\n",
    "    if k not in processing:\n",
    "        print(\"Cancelling\", k)\n",
    "        client.cancel(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in futures.items():\n",
    "    for f in v.copy():\n",
    "        if f.done():\n",
    "            done_keys.append(f.key)\n",
    "            f.release()\n",
    "            v.remove(f)\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(client)\n",
    "to_dec = []\n",
    "for k, v in client.refcount.items():\n",
    "    if k in done_keys:\n",
    "        print(k, v)\n",
    "        to_dec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in to_dec:\n",
    "    client._dec_ref(k)\n",
    "client.refcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.refcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsh {out_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in done_keys:\n",
    "    try:\n",
    "        client.cancel(key)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(n=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "#### Trying without dask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs = []\n",
    "for i, (exp_num, row) in enumerate(ec1_subset.iterrows()):\n",
    "    exp_conf = dict(row)\n",
    "    for ini_file in ini_files:\n",
    "        func = partial(run_inversion, data_path=data_path, out_path=out_path, exp_num=exp_num, exp_conf=exp_conf, ini_file=ini_file, sample_kwargs=sample_kwargs)\n",
    "        funcs.append(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = partial(funcs[0], sample_kwargs=(sample_kwargs | {\"progressbar\": True}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = funcs[0].keywords[\"out_path\"]\n",
    "!ls -R {out_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "This doesn't seem to be writing anything out... it looks like the ZarrTrace feature doesn't work with the numpyro sampler.\n",
    "\n",
    "Let's test on another config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec1_subset2 = ec1_subset.loc[(ec1_subset.offset == False) & (ec1_subset.bias == 0.0) & (ec1_subset.noise != 0.0)].sort_values([\"noise\", \"pefo\"])\n",
    "ec1_subset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 97\n",
    "exp_conf = dict(ec1_subset.loc[exp_num])\n",
    "ini_file = ini_files[0]\n",
    "result = run_inversion(data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num,\n",
    "                       exp_conf=exp_conf,\n",
    "                       ini_file=ini_file,\n",
    "                       sample_kwargs=sample_kwargs | {\"progressbar\": True, \"tune\": 100, \"draws\": 100},\n",
    "                       error_noise=0.6,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = run_inversion(data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num,\n",
    "                       exp_conf=exp_conf,\n",
    "                       ini_file=ini_file,\n",
    "                       sample_kwargs=sample_kwargs | {\"progressbar\": True, \"tune\": 100, \"draws\": 100, \"blas_cores\": 4},\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.retrieve import *\n",
    "\n",
    "flux_obj = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fix, axs = plt.subplots(1, 3, figsize=(15,7))\n",
    "vmin, vmax = -39, -28.5\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "flux_smoothed = bf_objs[0].interpolate(bf_objs[0].project(flux_obj.data.flux.isel(time=0), normalise=True))\n",
    "np.log(flux_smoothed * (bf_objs[0].flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "#np.log(flux_obj.data.flux.isel(time=0)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0], vmin=vmin, vmax=vmax)\n",
    "\n",
    "np.log(bf_objs[0].interpolate(result2.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[1], vmin=vmin, vmax=vmax)\n",
    "np.log(bf_objs[0].flux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[2], vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(flux_obj.data.flux.isel(time=0)).plot(vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "Let's try the easiest scenario: no noise, no bias:\n",
    "- 117: pefo True, no offset\n",
    "- 118: pefo False, no offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 117\n",
    "exp_conf = dict(ec1_subset.loc[exp_num])\n",
    "ini_file = ini_files[0]\n",
    "\n",
    "result3 = run_inversion(data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num,\n",
    "                       exp_conf=exp_conf,\n",
    "                       ini_file=ini_file,\n",
    "                       sample_kwargs=sample_kwargs | {\"progressbar\": True, \"tune\": 200, \"draws\": 400, \"blas_cores\": 8},\n",
    "                       error_noise=0.6,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this run, I've added the option to reparameterise log normals\n",
    "exp_num = 118\n",
    "exp_conf = dict(ec1_subset.loc[exp_num])\n",
    "ini_file = ini_files[0]\n",
    "\n",
    "result4 = run_inversion(data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num,\n",
    "                       exp_conf=exp_conf,\n",
    "                       ini_file=ini_file,\n",
    "                       sample_kwargs=sample_kwargs | {\"progressbar\": True, \"tune\": 200, \"draws\": 400, \"blas_cores\": 8},\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15,15))\n",
    "vmin, vmax = -39, -28\n",
    "\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "\n",
    "lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "# plot prior\n",
    "np.log(bf_objs[0].flux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 0], vmin=vmin, vmax=vmax, label=\"prior\")\n",
    "axs[0, 0].set_title(\"prior\")\n",
    "\n",
    "# plot true\n",
    "flux_smoothed = bf_objs[0].interpolate(bf_objs[0].project(flux_obj.data.flux.isel(time=0), normalise=True))\n",
    "np.log(flux_smoothed * (bf_objs[0].flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 1], vmin=vmin, vmax=vmax, label=\"true, smoothed to basis, masked by prior\")\n",
    "#np.log(flux_obj.data.flux.isel(time=0) * (bf_objs[0].flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 1], vmin=vmin, vmax=vmax)\n",
    "axs[0, 1].set_title(\"true\")\n",
    "\n",
    "# plot pefo true\n",
    "np.log(bf_objs[0].interpolate(result3.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[1, 0], vmin=vmin, vmax=vmax, label=\"no noise, no bias, pefo True, no offset\")\n",
    "axs[1, 0].set_title(\"exp. 117, no noise, pefo True, offset True (by mistake)\")\n",
    "\n",
    "# plot pefo false\n",
    "np.log(bf_objs[0].interpolate(result4.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[1, 1], vmin=vmin, vmax=vmax, label=\"no noise, no bias, pefo False, no offset\")\n",
    "axs[1, 1].set_title(\"exp. 118\")\n",
    "\n",
    "# plot country borders\n",
    "for ax in axs.flat:\n",
    "    world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "Let's run the same number of samples for these two model setups for the data with 2.0 sigma scaling (i.e. max noise)\n",
    "- 97: 2.0 noise, pefo=True\n",
    "- 99: 2.0 noise, pefo=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 97\n",
    "exp_conf = dict(ec1_subset.loc[exp_num])\n",
    "ini_file = ini_files[0]\n",
    "\n",
    "result6 = run_inversion(data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num,\n",
    "                       exp_conf=exp_conf,\n",
    "                       ini_file=ini_file,\n",
    "                       sample_kwargs=sample_kwargs | {\"progressbar\": True, \"tune\": 200, \"draws\": 400, \"blas_cores\": 8},\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 99\n",
    "exp_conf = dict(ec1_subset.loc[exp_num])\n",
    "ini_file = ini_files[0]\n",
    "\n",
    "result5 = run_inversion(data_path=data_path, \n",
    "                       out_path=out_path, \n",
    "                       exp_num=exp_num,\n",
    "                       exp_conf=exp_conf,\n",
    "                       ini_file=ini_file,\n",
    "                       sample_kwargs=sample_kwargs | {\"progressbar\": True, \"tune\": 200, \"draws\": 400, \"blas_cores\": 8},\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15,22))\n",
    "vmin, vmax = -39, -28\n",
    "\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "\n",
    "lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "# plot prior\n",
    "np.log(bf_objs[0].flux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 0], vmin=vmin, vmax=vmax, label=\"prior\")\n",
    "axs[0, 0].set_title(\"prior\")\n",
    "\n",
    "# plot true\n",
    "flux_smoothed = bf_objs[0].interpolate(bf_objs[0].project(flux_obj.data.flux.isel(time=0), normalise=True))\n",
    "#np.log(flux_smoothed * (bf_objs[0].flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 1], vmin=vmin, vmax=vmax, label=\"true, smoothed to basis, masked by prior\")\n",
    "np.log(flux_obj.data.flux.isel(time=0) * (bf_objs[0].flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 1], vmin=vmin, vmax=vmax)\n",
    "axs[0, 1].set_title(\"true\")\n",
    "\n",
    "# plot pefo true\n",
    "np.log(bf_objs[0].interpolate(result3.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[1, 0], vmin=vmin, vmax=vmax, label=\"no noise, no bias, pefo True, no offset\")\n",
    "axs[1, 0].set_title(\"exp. 117, no noise, pefo True, offset True (by mistake)\")\n",
    "\n",
    "# plot pefo false\n",
    "np.log(bf_objs[0].interpolate(result4.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[1, 1], vmin=vmin, vmax=vmax, label=\"no noise, no bias, pefo False, no offset\")\n",
    "axs[1, 1].set_title(\"exp. 118, no noise, pefo False\")\n",
    "\n",
    "# plot pefo true, with noise\n",
    "np.log(bf_objs[0].interpolate(result6.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[2, 0], vmin=vmin, vmax=vmax, label=\"2.0 noise, no bias, pefo True, no offset\")\n",
    "axs[2, 0].set_title(\"exp. 97, 2.0 noise, pefo True\")\n",
    "\n",
    "# plot pefo false, with noise\n",
    "np.log(bf_objs[0].interpolate(result5.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[2, 1], vmin=vmin, vmax=vmax, label=\"2.0 noise, no bias, pefo False, no offset\")\n",
    "axs[2, 1].set_title(\"exp. 99, 2.0 noise, pefo False\")\n",
    "\n",
    "# plot country borders\n",
    "for ax in axs.flat:\n",
    "    world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### Runs for more years\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lsh {out_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "TraceInfo = namedtuple(\"TraceInfo\", \"exp_num, title, trace\")\n",
    "\n",
    "def get_trace_info(year: int) -> list[TraceInfo]:\n",
    "    traces = [az.InferenceData.from_netcdf(f) for f in out_path.glob(f\"{year}*.nc\")]\n",
    "    exp_nums = [p.name.split(\"_\")[-2] for p in list(out_path.glob(f\"{year}*.nc\"))]\n",
    "    titles = [f\"Experiment {exp_num}: noise {row['noise']}, pefo {row['pefo']}\" for exp_num, row in ec1_subset2.loc[map(int, exp_nums)].iterrows()]\n",
    "    return [TraceInfo(exp_num, title, trace) for exp_num, title, trace in zip(exp_nums, titles, traces)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "traces_2017 = [az.InferenceData.from_netcdf(f) for f in out_path.glob(\"2017*.nc\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps_2017 = [p.name.split(\"_\")[-2] for p in list(out_path.glob(\"2017*.nc\"))]\n",
    "exps_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec1_subset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [f\"Experiment {exp_num}: noise {row['noise']}, pefo {row['pefo']}\" for exp_num, row in ec1_subset2.loc[map(int, exps_2017)].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_2017[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_obj = bf_objs[2]\n",
    "bf_obj.flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp_flux(year):\n",
    "    with xr.open_datatree(data_path / f\"sf6_{year}-01-01_4h-no-basis-no-filt_merged-data.zarr.zip\", engine=\"zarr\") as dt:\n",
    "        return mean_fp_x_flux(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(bf_obj.labels_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(15,29))\n",
    "vmin, vmax = -39, -28\n",
    "\n",
    "lat_slice = slice(37, None)\n",
    "lon_slice = slice(-14, 25)\n",
    "\n",
    "\n",
    "lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "# plot prior\n",
    "np.log(bf_obj.flux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs.flat[0], vmin=vmin, vmax=vmax, label=\"prior\")\n",
    "axs.flat[0].set_title(\"prior\")\n",
    "\n",
    "# plot true\n",
    "flux_smoothed = bf_obj.interpolate(bf_obj.project(flux_obj.data.flux.isel(time=0), normalise=True))\n",
    "np.log(flux_smoothed * (bf_obj.flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 1], vmin=vmin, vmax=vmax, label=\"true, smoothed to basis, masked by prior\")\n",
    "#np.log(flux_obj.data.flux.isel(time=0) * (bf_obj.flux > 0).astype(float)).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs.flat[1], vmin=vmin, vmax=vmax)\n",
    "axs.flat[1].set_title(\"true\")\n",
    "\n",
    "for trace, title, ax in zip(traces_2017, titles, axs.flat[2:]):\n",
    "    np.log(bf_obj.interpolate(trace.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=ax, vmin=vmin, vmax=vmax, label=\"no noise, no bias, pefo True, no offset\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "np.log(fpflux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs.flat[-2], label=\"mean fp x flux\")\n",
    "axs.flat[-2].set_title(\"mean fp x flux\")\n",
    "\n",
    "bf_obj.plot(shuffle=True, ax=axs.flat[-1])\n",
    "axs.flat[-1].set_title(\"basis functions\")\n",
    "\n",
    "# plot country borders\n",
    "for ax in axs.flat:\n",
    "    world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "    ax.set_xlim(lon_min, lon_max)\n",
    "    ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "#### 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "trace_info_2015 = get_trace_info(2015)\n",
    "bf_obj_2015 = bf_objs[0]\n",
    "fpflux_2015 = get_fp_flux(2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2015\n",
    "flux_2015 = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\").data.flux.sel(time=slice(f\"{year}-01-01\", f\"{year + 1}-01-01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiments(trace_info, bf_obj, fpflux, flux, year, smooth_true: bool = True, mask_true: bool = True):\n",
    "    fig, axs = plt.subplots(4, 2, figsize=(15,29))\n",
    "    fig.suptitle(f\"Experiments for {year}\")\n",
    "    \n",
    "    vmin, vmax = -39, -28\n",
    "\n",
    "#    lat_slice = slice(37, None)\n",
    "#    lon_slice = slice(-14, 25)\n",
    "    lat_slice = slice(None, None)\n",
    "    lon_slice = slice(None, None)\n",
    "\n",
    "    lat_min, lat_max = lat_slice.start, lat_slice.stop\n",
    "    lon_min, lon_max = lon_slice.start, lon_slice.stop\n",
    "\n",
    "    # plot prior\n",
    "    np.log(bf_obj.flux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs.flat[0], vmin=vmin, vmax=vmax, label=\"prior\")\n",
    "    axs.flat[0].set_title(\"prior\")\n",
    "\n",
    "    # plot true\n",
    "    mask = (bf_obj.flux > 0).astype(float) if mask_true else 1.0\n",
    "\n",
    "    if \"time\" in flux.dims:\n",
    "        flux = flux.isel(time=0)\n",
    "    \n",
    "    if smooth_true:\n",
    "        flux_smoothed = bf_obj.interpolate(bf_obj.project(flux, normalise=True))\n",
    "        np.log(flux_smoothed * mask).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs[0, 1], vmin=vmin, vmax=vmax, label=\"true, smoothed to basis, masked by prior\")\n",
    "    else:\n",
    "        np.log(flux * mask).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs.flat[1], vmin=vmin, vmax=vmax)\n",
    "    axs.flat[1].set_title(\"true\")\n",
    "\n",
    "    for (_, title, trace), ax in zip(trace_info, axs.flat[2:]):\n",
    "        np.log(bf_obj.interpolate(trace.posterior.x.mean([\"chain\", \"draw\"]), flux=True)).sel(lat=lat_slice, lon=lon_slice).plot(ax=ax, vmin=vmin, vmax=vmax, label=\"no noise, no bias, pefo True, no offset\")\n",
    "        ax.set_title(title)\n",
    "\n",
    "    np.log(fpflux).sel(lat=lat_slice, lon=lon_slice).plot(ax=axs.flat[-2], label=\"mean fp x flux\")\n",
    "    axs.flat[-2].set_title(\"mean fp x flux\")\n",
    "\n",
    "    bf_obj.plot(shuffle=True, ax=axs.flat[-1])\n",
    "    axs.flat[-1].set_title(\"basis functions\")\n",
    "\n",
    "    # plot country borders\n",
    "    for ax in axs.flat:\n",
    "        world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "        ax.set_xlim(lon_min, lon_max)\n",
    "        ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_experiments(trace_info_2015, bf_obj_2015, fpflux_2015, flux_2015, 2015, smooth_true=False, mask_true=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "#### 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2016\n",
    "plot_args_2016 = dict(\n",
    "    trace_info = get_trace_info(year),\n",
    "    bf_obj = bf_objs[year - 2015],\n",
    "    fpflux = get_fp_flux(year),\n",
    "    flux = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\").data.flux.sel(time=slice(f\"{year}-01-01\", f\"{year + 1}-01-01\")),\n",
    ")\n",
    "plot_experiments(**plot_args_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "#### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "plot_args_2018 = dict(\n",
    "    trace_info = get_trace_info(year),\n",
    "    bf_obj = bf_objs[year - 2015],\n",
    "    fpflux = get_fp_flux(year),\n",
    "    flux = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\").data.flux.sel(time=slice(f\"{year}-01-01\", f\"{year + 1}-01-01\")),\n",
    "    year = year,\n",
    ")\n",
    "plot_experiments(**plot_args_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178",
   "metadata": {},
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "plot_args_2019 = dict(\n",
    "    trace_info = get_trace_info(year),\n",
    "    bf_obj = bf_objs[year - 2015],\n",
    "    fpflux = get_fp_flux(year),\n",
    "    flux = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\").data.flux.sel(time=slice(f\"{year}-01-01\", f\"{year + 1}-01-01\")),\n",
    "    year = year,\n",
    ")\n",
    "plot_experiments(**plot_args_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "### Country totals for 2015-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg_inversions.array_ops import sparse_xr_dot, align_sparse_lat_lon\n",
    "from openghg_inversions.postprocessing.countries import Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "Countries.from_file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = Countries.from_file(\"/group/chem/acrg/LPDM/countries/country_EUROPE_EEZ_PARIS_gapfilled.nc\", country_code=\"alpha3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries.get_x_to_country_mat??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openghg.util import molar_mass\n",
    "\n",
    "sf6_mm = molar_mass(\"sf6\")\n",
    "print(sf6_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = get_flux(species=\"sf6\", domain=\"europe\", source=\"edgar-annual-total\").data.flux\n",
    "\n",
    "_, flux_aligned = xr.align(countries.area_grid, flux, join=\"override\")\n",
    "true_country_totals = sparse_xr_dot(countries.matrix, countries.area_grid * flux_aligned).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux * 1e-3 / sf6_mm * (365.25 * 24 * 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 1 is \"OCEAN\", which includes part of North America...\n",
    "true_country_df = (true_country_totals / sf6_mm * 1e-3 * (365.25 * 24 * 3600)).to_series().unstack().T.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_country_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [get_trace_info(year) for year in range(2015, 2025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_flux_dict = defaultdict(list)\n",
    "for trace, year in zip(traces, range(2015, 2025)):\n",
    "    for ti in trace:\n",
    "        post_flux_dict[ti.exp_num].append(ti.trace.posterior.x.mean([\"draw\", \"chain\"]).expand_dims({\"time\": [f\"{year}-01-01\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bfo in bf_objs:\n",
    "    print(bfo.flux.time.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_objs[0].interpolate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_flux_concat = {k : xr.concat([bfo.interpolate(ds * bfo.flux.squeeze(\"time\")) for ds, bfo in zip(v, bf_objs)], dim=\"time\") for k, v in post_flux_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_flux_concat[\"97\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_country_totals = {}\n",
    "for k, v in post_flux_concat.items():\n",
    "    _, flux_aligned = xr.align(countries.area_grid, v, join=\"override\")\n",
    "    post_country_totals[k] = sparse_xr_dot(countries.matrix, countries.area_grid * flux_aligned).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_country_dfs = {k: (v / sf6_mm * 1e-3 * (365.25 * 24 * 3600)).as_numpy().to_series().unstack().T.iloc[:, 1:] for k, v in post_country_totals.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_country_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [ti.title for ti in traces[0]]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in post_country_dfs.values():\n",
    "    df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_country_df[\"DEU\"].index)\n",
    "print(post_country_dfs[\"39\"][\"DEU\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "\n",
    "for pcdf, title in zip(post_country_dfs.values(), titles):\n",
    "    pcdf[\"DEU\"].plot(ax=ax, label=title)\n",
    "\n",
    "true_country_df[\"DEU\"].iloc[2:7].plot(ax=ax, label=\"true\")\n",
    "\n",
    "fig.legend()\n",
    "ax.set_title(\"Germany country totals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203",
   "metadata": {},
   "source": [
    "...forgot to do x to country matrix... need to do this year by year..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ds = xr.open_dataset(\"/group/chem/acrg/LPDM/countries/country_EUROPE_EEZ_PARIS_gapfilled.nc\")\n",
    "country_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "lon_min = country_ds.lon.min().values\n",
    "lon_max = country_ds.lon.max().values\n",
    "lat_min = country_ds.lat.min().values\n",
    "lat_max = country_ds.lat.max().values\n",
    "\n",
    "(country_ds.country == 0).plot(ax=ax)\n",
    "world.boundary.plot(ax=ax, linewidth=0.6, edgecolor='white')  # or .plot(facecolor='none')\n",
    "ax.set_xlim(lon_min, lon_max)\n",
    "ax.set_ylim(lat_min, lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ds.country_code.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log((country_ds.country == 0) * flux.isel(time=0)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
